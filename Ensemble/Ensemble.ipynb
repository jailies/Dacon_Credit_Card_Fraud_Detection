{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "1sbYGESQLYai",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sbYGESQLYai",
        "outputId": "9630abe2-fbcb-47bc-eb0b-19610b4fad76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시드고정"
      ],
      "metadata": {
        "id": "TZHRvAOUVksG"
      },
      "id": "TZHRvAOUVksG"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(41) # Seed 고정"
      ],
      "metadata": {
        "id": "h9_i0pkWN8E-"
      },
      "id": "h9_i0pkWN8E-",
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
      "metadata": {
        "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import sklearn\n",
        "import sklearn.preprocessing as sp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "lvrnO3SgStYn"
      },
      "id": "lvrnO3SgStYn",
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EllipticEnvelope"
      ],
      "metadata": {
        "id": "8WqdO_IQIgl8"
      },
      "id": "8WqdO_IQIgl8"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/train.csv')\n",
        "val_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/val.csv')\n",
        "test = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/test.csv')\n",
        "\n",
        "print(train_df.shape, val_df.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YncBj4Y_R8-7",
        "outputId": "dafb279a-3a3e-4099-9fe6-8444e7189548"
      },
      "id": "YncBj4Y_R8-7",
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(113842, 31) (28462, 32) (142503, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.datasets import make_moons, make_blobs\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.linear_model import SGDOneClassSVM\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "Z0_sk7n2TCgV"
      },
      "id": "Z0_sk7n2TCgV",
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.covariance import EllipticEnvelope"
      ],
      "metadata": {
        "id": "URYLbXCBZGR3"
      },
      "id": "URYLbXCBZGR3",
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset은 Label이 존재하지 않음\n",
        "train_df = train_df.drop(columns=['ID']) # Input Data"
      ],
      "metadata": {
        "id": "XkR2v3OTb639"
      },
      "id": "XkR2v3OTb639",
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# StandardScaler 선언 및 Fitting\n",
        "sdscaler = StandardScaler()\n",
        "sdscaler.fit(train_df)\n",
        "\n",
        "# 데이터 변환\n",
        "sdscaled_data = sdscaler.transform(train_df)\n",
        "\n",
        "# 데이터 프레임으로 저장\n",
        "sdscaled_data = pd.DataFrame(sdscaled_data)"
      ],
      "metadata": {
        "id": "DRJkUaCZZ8-R"
      },
      "id": "DRJkUaCZZ8-R",
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdscaled_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "vnRXPljsaJy0",
        "outputId": "deaa10f6-38ff-4e5b-a8a7-5d3d94e80fbf"
      },
      "id": "vnRXPljsaJy0",
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0      -0.696317 -0.812481  1.178089  0.271798 -0.368309  1.349308  0.652511   \n",
              "1      -0.495358 -0.112967  1.191305 -0.608173 -0.007886  0.933476  0.192541   \n",
              "2      -0.218427  0.580982  0.755819 -0.116154  0.307498 -0.026206  0.390690   \n",
              "3      -0.330318  0.858041  0.711241 -0.345477  0.693558  0.317890  0.925884   \n",
              "4      -0.458462  0.172537 -0.082109 -0.189262  1.951788  2.793282  0.302611   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "113837 -6.415477  6.169704 -5.669274 -1.774215 -3.354334 -1.051879 -3.021567   \n",
              "113838  0.965967 -0.087719 -0.674497  1.069597 -0.026160 -0.465046  0.153202   \n",
              "113839 -0.124097  0.430608  0.260596 -0.325094  0.178465 -1.013702  0.767042   \n",
              "113840  0.061566  0.563104 -0.371251 -0.524503  0.826191 -0.181210  0.670168   \n",
              "113841 -6.089697  6.099426 -6.576552 -1.460036 -3.923101 -1.963042 -4.089330   \n",
              "\n",
              "              7         8         9   ...        20        21        22  \\\n",
              "0       0.210988 -1.380804  0.188955  ...  0.345210  1.066111  1.431834   \n",
              "1       0.320444 -1.264291 -0.056156  ... -0.148280  0.007285 -0.297148   \n",
              "2       0.221649 -0.517216 -0.351540  ... -0.286720 -0.773425 -0.039431   \n",
              "3      -3.209968  0.563699  1.161326  ...  2.693508 -1.402900  0.092477   \n",
              "4       0.719980 -0.355977 -0.387965  ... -0.099976 -0.370382 -0.319020   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "113837  4.640268  4.468824  8.074160  ... -1.306812 -2.162159  1.402375   \n",
              "113838 -0.208020  0.610333  0.107995  ...  0.201179  0.876793 -0.064140   \n",
              "113839 -0.171877  0.098905 -0.270613  ... -0.315282 -0.710635  0.441651   \n",
              "113840  0.099151 -0.184366 -0.618512  ... -0.433467 -1.117010  0.081219   \n",
              "113841  6.164311  1.749605  4.061261  ...  0.297364  0.154545  1.597020   \n",
              "\n",
              "              23        24        25        26        27        28        29  \n",
              "0      -1.136246 -0.633230 -0.288586 -0.137969 -0.166355  1.188563 -1.990839  \n",
              "1      -1.938909  1.241563 -0.460803  0.157587  0.173998  0.143876 -1.990839  \n",
              "2      -0.611606 -0.450853  0.220818  0.635984  0.229098 -0.346737 -1.990818  \n",
              "3      -1.070930 -0.801717 -0.106743 -3.020466 -3.046182 -0.194717 -1.990712  \n",
              "4       1.671161  0.714378 -0.798093  0.029990  0.401295  0.019821 -1.990712  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "113837 -2.067159  3.432320  0.667510  5.233857  3.463283 -0.321352  1.645815  \n",
              "113838 -0.086360  0.605158 -0.958774  0.046304 -0.113892 -0.116108  1.645857  \n",
              "113839  0.614549 -1.078550  0.235849  0.329761  0.229617 -0.339285  1.645899  \n",
              "113840  0.171137 -0.841334  0.258583  0.546111  0.194625 -0.350749  1.645962  \n",
              "113841 -0.839255  2.759503  0.520457  2.362643  2.314445 -0.358610  1.645983  \n",
              "\n",
              "[113842 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3471187d-7e9b-4012-8cce-0661fd19a8c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.696317</td>\n",
              "      <td>-0.812481</td>\n",
              "      <td>1.178089</td>\n",
              "      <td>0.271798</td>\n",
              "      <td>-0.368309</td>\n",
              "      <td>1.349308</td>\n",
              "      <td>0.652511</td>\n",
              "      <td>0.210988</td>\n",
              "      <td>-1.380804</td>\n",
              "      <td>0.188955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.345210</td>\n",
              "      <td>1.066111</td>\n",
              "      <td>1.431834</td>\n",
              "      <td>-1.136246</td>\n",
              "      <td>-0.633230</td>\n",
              "      <td>-0.288586</td>\n",
              "      <td>-0.137969</td>\n",
              "      <td>-0.166355</td>\n",
              "      <td>1.188563</td>\n",
              "      <td>-1.990839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.495358</td>\n",
              "      <td>-0.112967</td>\n",
              "      <td>1.191305</td>\n",
              "      <td>-0.608173</td>\n",
              "      <td>-0.007886</td>\n",
              "      <td>0.933476</td>\n",
              "      <td>0.192541</td>\n",
              "      <td>0.320444</td>\n",
              "      <td>-1.264291</td>\n",
              "      <td>-0.056156</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.148280</td>\n",
              "      <td>0.007285</td>\n",
              "      <td>-0.297148</td>\n",
              "      <td>-1.938909</td>\n",
              "      <td>1.241563</td>\n",
              "      <td>-0.460803</td>\n",
              "      <td>0.157587</td>\n",
              "      <td>0.173998</td>\n",
              "      <td>0.143876</td>\n",
              "      <td>-1.990839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.218427</td>\n",
              "      <td>0.580982</td>\n",
              "      <td>0.755819</td>\n",
              "      <td>-0.116154</td>\n",
              "      <td>0.307498</td>\n",
              "      <td>-0.026206</td>\n",
              "      <td>0.390690</td>\n",
              "      <td>0.221649</td>\n",
              "      <td>-0.517216</td>\n",
              "      <td>-0.351540</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.286720</td>\n",
              "      <td>-0.773425</td>\n",
              "      <td>-0.039431</td>\n",
              "      <td>-0.611606</td>\n",
              "      <td>-0.450853</td>\n",
              "      <td>0.220818</td>\n",
              "      <td>0.635984</td>\n",
              "      <td>0.229098</td>\n",
              "      <td>-0.346737</td>\n",
              "      <td>-1.990818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.330318</td>\n",
              "      <td>0.858041</td>\n",
              "      <td>0.711241</td>\n",
              "      <td>-0.345477</td>\n",
              "      <td>0.693558</td>\n",
              "      <td>0.317890</td>\n",
              "      <td>0.925884</td>\n",
              "      <td>-3.209968</td>\n",
              "      <td>0.563699</td>\n",
              "      <td>1.161326</td>\n",
              "      <td>...</td>\n",
              "      <td>2.693508</td>\n",
              "      <td>-1.402900</td>\n",
              "      <td>0.092477</td>\n",
              "      <td>-1.070930</td>\n",
              "      <td>-0.801717</td>\n",
              "      <td>-0.106743</td>\n",
              "      <td>-3.020466</td>\n",
              "      <td>-3.046182</td>\n",
              "      <td>-0.194717</td>\n",
              "      <td>-1.990712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.458462</td>\n",
              "      <td>0.172537</td>\n",
              "      <td>-0.082109</td>\n",
              "      <td>-0.189262</td>\n",
              "      <td>1.951788</td>\n",
              "      <td>2.793282</td>\n",
              "      <td>0.302611</td>\n",
              "      <td>0.719980</td>\n",
              "      <td>-0.355977</td>\n",
              "      <td>-0.387965</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.099976</td>\n",
              "      <td>-0.370382</td>\n",
              "      <td>-0.319020</td>\n",
              "      <td>1.671161</td>\n",
              "      <td>0.714378</td>\n",
              "      <td>-0.798093</td>\n",
              "      <td>0.029990</td>\n",
              "      <td>0.401295</td>\n",
              "      <td>0.019821</td>\n",
              "      <td>-1.990712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113837</th>\n",
              "      <td>-6.415477</td>\n",
              "      <td>6.169704</td>\n",
              "      <td>-5.669274</td>\n",
              "      <td>-1.774215</td>\n",
              "      <td>-3.354334</td>\n",
              "      <td>-1.051879</td>\n",
              "      <td>-3.021567</td>\n",
              "      <td>4.640268</td>\n",
              "      <td>4.468824</td>\n",
              "      <td>8.074160</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.306812</td>\n",
              "      <td>-2.162159</td>\n",
              "      <td>1.402375</td>\n",
              "      <td>-2.067159</td>\n",
              "      <td>3.432320</td>\n",
              "      <td>0.667510</td>\n",
              "      <td>5.233857</td>\n",
              "      <td>3.463283</td>\n",
              "      <td>-0.321352</td>\n",
              "      <td>1.645815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113838</th>\n",
              "      <td>0.965967</td>\n",
              "      <td>-0.087719</td>\n",
              "      <td>-0.674497</td>\n",
              "      <td>1.069597</td>\n",
              "      <td>-0.026160</td>\n",
              "      <td>-0.465046</td>\n",
              "      <td>0.153202</td>\n",
              "      <td>-0.208020</td>\n",
              "      <td>0.610333</td>\n",
              "      <td>0.107995</td>\n",
              "      <td>...</td>\n",
              "      <td>0.201179</td>\n",
              "      <td>0.876793</td>\n",
              "      <td>-0.064140</td>\n",
              "      <td>-0.086360</td>\n",
              "      <td>0.605158</td>\n",
              "      <td>-0.958774</td>\n",
              "      <td>0.046304</td>\n",
              "      <td>-0.113892</td>\n",
              "      <td>-0.116108</td>\n",
              "      <td>1.645857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113839</th>\n",
              "      <td>-0.124097</td>\n",
              "      <td>0.430608</td>\n",
              "      <td>0.260596</td>\n",
              "      <td>-0.325094</td>\n",
              "      <td>0.178465</td>\n",
              "      <td>-1.013702</td>\n",
              "      <td>0.767042</td>\n",
              "      <td>-0.171877</td>\n",
              "      <td>0.098905</td>\n",
              "      <td>-0.270613</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.315282</td>\n",
              "      <td>-0.710635</td>\n",
              "      <td>0.441651</td>\n",
              "      <td>0.614549</td>\n",
              "      <td>-1.078550</td>\n",
              "      <td>0.235849</td>\n",
              "      <td>0.329761</td>\n",
              "      <td>0.229617</td>\n",
              "      <td>-0.339285</td>\n",
              "      <td>1.645899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113840</th>\n",
              "      <td>0.061566</td>\n",
              "      <td>0.563104</td>\n",
              "      <td>-0.371251</td>\n",
              "      <td>-0.524503</td>\n",
              "      <td>0.826191</td>\n",
              "      <td>-0.181210</td>\n",
              "      <td>0.670168</td>\n",
              "      <td>0.099151</td>\n",
              "      <td>-0.184366</td>\n",
              "      <td>-0.618512</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.433467</td>\n",
              "      <td>-1.117010</td>\n",
              "      <td>0.081219</td>\n",
              "      <td>0.171137</td>\n",
              "      <td>-0.841334</td>\n",
              "      <td>0.258583</td>\n",
              "      <td>0.546111</td>\n",
              "      <td>0.194625</td>\n",
              "      <td>-0.350749</td>\n",
              "      <td>1.645962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113841</th>\n",
              "      <td>-6.089697</td>\n",
              "      <td>6.099426</td>\n",
              "      <td>-6.576552</td>\n",
              "      <td>-1.460036</td>\n",
              "      <td>-3.923101</td>\n",
              "      <td>-1.963042</td>\n",
              "      <td>-4.089330</td>\n",
              "      <td>6.164311</td>\n",
              "      <td>1.749605</td>\n",
              "      <td>4.061261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.297364</td>\n",
              "      <td>0.154545</td>\n",
              "      <td>1.597020</td>\n",
              "      <td>-0.839255</td>\n",
              "      <td>2.759503</td>\n",
              "      <td>0.520457</td>\n",
              "      <td>2.362643</td>\n",
              "      <td>2.314445</td>\n",
              "      <td>-0.358610</td>\n",
              "      <td>1.645983</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>113842 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3471187d-7e9b-4012-8cce-0661fd19a8c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3471187d-7e9b-4012-8cce-0661fd19a8c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3471187d-7e9b-4012-8cce-0661fd19a8c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_label(model_pred):\n",
        "    # IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n",
        "    model_pred = np.where(model_pred == 1, 0, model_pred)\n",
        "    model_pred = np.where(model_pred == -1, 1, model_pred)\n",
        "    return model_pred"
      ],
      "metadata": {
        "id": "HL0TnpjdTEp7"
      },
      "id": "HL0TnpjdTEp7",
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_x = val_df.drop(columns=['ID', 'Class']) # Input Data"
      ],
      "metadata": {
        "id": "BEHFgOytoV_q"
      },
      "id": "BEHFgOytoV_q",
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler 선언 및 Fitting\n",
        "StandardScaler = StandardScaler()\n",
        "StandardScaler.fit(val_x)\n",
        "\n",
        "# 데이터 변환\n",
        "val_StandardScaled_data = StandardScaler.transform(val_x)\n",
        "\n",
        "# 데이터 프레임으로 저장\n",
        "val_StandardScaled_data = pd.DataFrame(val_StandardScaled_data)"
      ],
      "metadata": {
        "id": "aVmr6AAsobdy"
      },
      "id": "aVmr6AAsobdy",
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model define, fit"
      ],
      "metadata": {
        "id": "clzEksUNVsTW"
      },
      "id": "clzEksUNVsTW"
    },
    {
      "cell_type": "code",
      "source": [
        "model = EllipticEnvelope(support_fraction = 0.994, contamination = 0.00112, random_state = 42)\n",
        "model.fit(sdscaled_data)\n",
        "val_y = val_df['Class'] # Label\n",
        "\n",
        "val_pred_E = model.predict(val_StandardScaled_data) # model prediction\n",
        "val_pred_E = get_pred_label(val_pred_E)\n",
        "val_score_E = f1_score(val_y, val_pred_E, average='macro')\n",
        "print(f'Validation F1 Score : [{val_score_E}]')\n",
        "print(classification_report(val_y, val_pred_E))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK_wWQeYTGeA",
        "outputId": "50e13e97-fe19-4bc4-8e2c-cfb2128f9bcc"
      },
      "id": "EK_wWQeYTGeA",
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score : [0.9236496787663914]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     28432\n",
            "           1       0.86      0.83      0.85        30\n",
            "\n",
            "    accuracy                           1.00     28462\n",
            "   macro avg       0.93      0.92      0.92     28462\n",
            "weighted avg       1.00      1.00      1.00     28462\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_E"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLFaBTWauPwO",
        "outputId": "5cae4642-fbeb-4bc4-afb0-2a53f4cbec40"
      },
      "id": "RLFaBTWauPwO",
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "4B0kO5wtV6uA"
      },
      "id": "4B0kO5wtV6uA"
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = test.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "BqOQT3k5TyBB"
      },
      "id": "BqOQT3k5TyBB",
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler 선언 및 Fitting\n",
        "StandardScaler = StandardScaler()\n",
        "StandardScaler.fit(test_x)\n",
        "\n",
        "# 데이터 변환\n",
        "test_StandardScaled_data = StandardScaler.transform(test_x)\n",
        "\n",
        "# 데이터 프레임으로 저장\n",
        "test_StandardScaled_data = pd.DataFrame(test_StandardScaled_data)"
      ],
      "metadata": {
        "id": "aEH7pSLIvpoJ"
      },
      "id": "aEH7pSLIvpoJ",
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best model : model \"EllipticEnvelope\"\n",
        "test_pred_E = model.predict(test_StandardScaled_data) # model prediction\n",
        "test_pred_E = get_pred_label(test_pred_E)"
      ],
      "metadata": {
        "id": "cinR2nmwTUOH"
      },
      "id": "cinR2nmwTUOH",
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_E"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmvhDTGetm5I",
        "outputId": "8d3b588e-9640-4ea3-e939-8da6b32b46d2"
      },
      "id": "wmvhDTGetm5I",
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_pred_E)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjjPMn9dFGdb",
        "outputId": "91debd5d-ccb4-46ed-9283-32d2a00e357b"
      },
      "id": "JjjPMn9dFGdb",
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142503"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Isolation Forest"
      ],
      "metadata": {
        "id": "8Zf5nVcjJj1y"
      },
      "id": "8Zf5nVcjJj1y"
    },
    {
      "cell_type": "code",
      "source": [
        "isf13 = IsolationForest(n_estimators=200, max_samples = 0.2, contamination=0.0011, max_features=1.0, random_state=0).fit(train_df.loc[:, ['V17', 'V14', 'V12', 'V10', 'V7', 'V3', 'V16']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYyWBEgQ-i6v",
        "outputId": "811d5a23-4e60-4a3d-ef52-0f43f3e805dc"
      },
      "id": "vYyWBEgQ-i6v",
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result13 = isf13.predict(val_x.loc[:, ['V17', 'V14', 'V12', 'V10', 'V7', 'V3', 'V16']])"
      ],
      "metadata": {
        "id": "O_0KQ3yR-Te_"
      },
      "id": "O_0KQ3yR-Te_",
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_score_I = f1_score(val_y, np.where(result13==1, 0, 1), average='macro')\n",
        "val_score_I"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16wqXedS-vr6",
        "outputId": "fa52aa2a-a52d-478d-fb65-b7270c215b9c"
      },
      "id": "16wqXedS-vr6",
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7879157743510873"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_I = np.where(result13==1, 0, 1)"
      ],
      "metadata": {
        "id": "Cmjse6UZFLyE"
      },
      "id": "Cmjse6UZFLyE",
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_I"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "absvj1bjFPIX",
        "outputId": "3c0a417e-7525-4298-fecc-49e958fc1be3"
      },
      "id": "absvj1bjFPIX",
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_I = isf13.predict(test_x.loc[:, ['V17', 'V14', 'V12', 'V10', 'V7', 'V3', 'V16']])"
      ],
      "metadata": {
        "id": "cfABfB59-6tX"
      },
      "id": "cfABfB59-6tX",
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_I = np.where(test_pred_I==1, 0, 1)\n",
        "test_pred_I"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKihQyj6_IPg",
        "outputId": "50f8c3d3-294c-46e7-eb7a-9504b71814ca"
      },
      "id": "fKihQyj6_IPg",
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_pred_I)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7UE1Ois_V2l",
        "outputId": "61af559f-ff3e-489e-e753-cb53c540a052"
      },
      "id": "C7UE1Ois_V2l",
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142503"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Drzt9bRm_Lic"
      },
      "id": "Drzt9bRm_Lic"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Class SVM"
      ],
      "metadata": {
        "id": "LCwU0LIhJpaU"
      },
      "id": "LCwU0LIhJpaU"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "metadata": {
        "id": "hNAqbet9KaFK"
      },
      "id": "hNAqbet9KaFK",
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_label(model, x, k):\n",
        "  prob = model.score_samples(x)\n",
        "  prob = torch.tensor(prob, dtype = torch.float)\n",
        "  topk_indices = torch.topk(prob, k = k, largest = False).indices\n",
        "\n",
        "  pred = torch.zeros(len(x), dtype = torch.long)\n",
        "  pred[topk_indices] = 1\n",
        "  return pred.tolist(), prob.tolist()"
      ],
      "metadata": {
        "id": "BjUDxd3h3Paz"
      },
      "id": "BjUDxd3h3Paz",
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/train.csv').drop(columns=['ID'])\n",
        "valid_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/val.csv').drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "ysRSQ6E6OZ3l"
      },
      "id": "ysRSQ6E6OZ3l",
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_scaler = StandardScaler()\n",
        "train_std = std_scaler.fit_transform(train_df)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "prcomponent = pca.fit_transform(train_std)"
      ],
      "metadata": {
        "id": "dkMzvXH_KEzH"
      },
      "id": "dkMzvXH_KEzH",
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principal = pd.DataFrame(data=prcomponent, columns = ['pca_v1', 'pca_v2', 'pca_v3'])"
      ],
      "metadata": {
        "id": "MN4vogYhKIK8"
      },
      "id": "MN4vogYhKIK8",
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = svm.OneClassSVM(gamma=0.0001, kernel='rbf', max_iter=100000, nu=0.001, verbose=True)\n",
        "model.fit(principal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKAAeQ2aKgd0",
        "outputId": "77612f17-e3f9-4885-8cf0-a279b4fc0171"
      },
      "id": "qKAAeQ2aKgd0",
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneClassSVM(gamma=0.0001, max_iter=100000, nu=0.001, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_std = std_scaler.fit_transform(valid_df)\n",
        "prcomponent_ = pca.fit_transform(valid_std)\n",
        "principal_ = pd.DataFrame(data=prcomponent_, columns = ['pca_v1', 'pca_v2', 'pca_v3'])"
      ],
      "metadata": {
        "id": "qz8KkQy9KkJ_"
      },
      "id": "qz8KkQy9KkJ_",
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab = valid_df.iloc[:,-1]"
      ],
      "metadata": {
        "id": "1Scfex4wr3B2"
      },
      "id": "1Scfex4wr3B2",
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_O, val_prob_O = get_pred_label(model, principal_, 28)\n",
        "val_score_O = f1_score(lab, val_pred_O, average='macro')\n",
        "print(f'Validation F1 Score : [{val_score_O}]')\n",
        "print(classification_report(lab, val_pred_O))\n",
        "tn, fp, fn, tp = confusion_matrix(lab, val_pred_O).ravel()\n",
        "print('tp : ', tp, ', fp : ', fp, ', tn : ', tn, ', fn : ', fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVJO9gpeNaeL",
        "outputId": "1cce1aea-b4fb-4e95-a324-543d94a30d04"
      },
      "id": "VVJO9gpeNaeL",
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score : [0.9137051774467988]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     28432\n",
            "           1       0.86      0.80      0.83        30\n",
            "\n",
            "    accuracy                           1.00     28462\n",
            "   macro avg       0.93      0.90      0.91     28462\n",
            "weighted avg       1.00      1.00      1.00     28462\n",
            "\n",
            "tp :  24 , fp :  4 , tn :  28428 , fn :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_O = np.array(val_pred_O)"
      ],
      "metadata": {
        "id": "OSIRQ6nUuq9W"
      },
      "id": "OSIRQ6nUuq9W",
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pred_label_(model_pred):\n",
        "    model_pred = np.where(model_pred == 1, 0, model_pred)\n",
        "    model_pred = np.where(model_pred == -1, 1, model_pred)\n",
        "    return model_pred"
      ],
      "metadata": {
        "id": "dQRtg-BS3-lk"
      },
      "id": "dQRtg-BS3-lk",
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_t = test_x.columns\n",
        "test_df_std = std_scaler.fit_transform(test_x)\n",
        "test_df_std_ = pd.DataFrame(data=test_df_std, columns=col_t)\n",
        "\n",
        "prcomponent_ = pca.fit_transform(test_df_std_)\n",
        "principal_ = pd.DataFrame(data=prcomponent_, columns = ['pca_v1', 'pca_v2', 'pca_v3'])\n",
        "\n",
        "test_pred_O = model.predict(principal_)\n",
        "test_pred_O = get_pred_label_(test_pred_O)"
      ],
      "metadata": {
        "id": "WyeA57rDQWVZ"
      },
      "id": "WyeA57rDQWVZ",
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP4ayY4Z4Ad-",
        "outputId": "89637167-1c48-43c9-bbb8-033636a1e5b2"
      },
      "id": "oP4ayY4Z4Ad-",
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto Encoder"
      ],
      "metadata": {
        "id": "uTNvRcy4-l20"
      },
      "id": "uTNvRcy4-l20"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "import os"
      ],
      "metadata": {
        "id": "2lp6i-ve-ldA"
      },
      "id": "2lp6i-ve-ldA",
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vda1HENs_dv4",
        "outputId": "9ce7348a-9e6d-4cf6-a21f-72f195231a83"
      },
      "id": "vda1HENs_dv4",
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT6tITcJ_f5I",
        "outputId": "e5cfdf3c-d0d4-4eb6-ad2f-34c11d205217"
      },
      "id": "gT6tITcJ_f5I",
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.device'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 400\n",
        "LR = 1e-2\n",
        "BS = 16384\n",
        "SEED = 41"
      ],
      "metadata": {
        "id": "S95eeF10-nZQ"
      },
      "id": "S95eeF10-nZQ",
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/train.csv').drop(columns=['ID'])\n",
        "valid_df = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/val.csv').drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "sWXmM9rQ-qbh"
      },
      "id": "sWXmM9rQ-qbh",
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, eval_mode):\n",
        "        self.df = df\n",
        "        self.eval_mode = eval_mode\n",
        "        if self.eval_mode:\n",
        "            self.labels = self.df['Class'].values\n",
        "            self.df = self.df.drop(columns=['Class']).values\n",
        "        else:\n",
        "            self.df = self.df.values\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if self.eval_mode:\n",
        "            self.x = self.df[index]\n",
        "            self.y = self.labels[index]\n",
        "            return torch.Tensor(self.x), self.y\n",
        "        else:\n",
        "            self.x = self.df[index]\n",
        "            return torch.Tensor(self.x)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "snsmeall-wW8"
      },
      "id": "snsmeall-wW8",
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(df=train_df, eval_mode=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=2)\n",
        "\n",
        "val_dataset = MyDataset(df = valid_df, eval_mode=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "86X9xPzw-ye4"
      },
      "id": "86X9xPzw-ye4",
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.Encoder = nn.Sequential(\n",
        "            nn.Linear(30,64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64,128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.Decoder = nn.Sequential(\n",
        "            nn.Linear(128,64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(64,30),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.Encoder(x)\n",
        "        x = self.Decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EuB7A59b-2w2"
      },
      "id": "EuB7A59b-2w2",
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        # Loss Function\n",
        "        self.criterion = nn.L1Loss().to(self.device)\n",
        "        \n",
        "    def fit(self, ):\n",
        "        self.model.to(self.device)\n",
        "        best_score = 0\n",
        "        for epoch in range(EPOCHS):\n",
        "            self.model.train()\n",
        "            train_loss = []\n",
        "            for x in iter(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                _x = self.model(x)\n",
        "                loss = self.criterion(x, _x)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss.append(loss.item())\n",
        "\n",
        "            score = self.validation(self.model, 0.95)\n",
        "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(score)\n",
        "\n",
        "            if best_score < score:\n",
        "                best_score = score\n",
        "                torch.save(model.module.state_dict(), './drive/MyDrive/신용카드 사기 데이콘/best_model.pth', _use_new_zipfile_serialization=False)\n",
        "    \n",
        "    def validation(self, eval_model, thr):\n",
        "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "        eval_model.eval()\n",
        "        pred = []\n",
        "        true = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in iter(self.val_loader):\n",
        "                x = x.float().to(self.device)\n",
        "\n",
        "                _x = self.model(x)\n",
        "                diff = cos(x, _x).cpu().tolist()\n",
        "                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "                pred += batch_pred\n",
        "                true += y.tolist()\n",
        "\n",
        "        return f1_score(true, pred, average='macro')\n",
        "        \n",
        "    def test(self, test_loader):\n",
        "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "        self.model.eval()\n",
        "        pred = []\n",
        "        pred_A = []  # initialize list to store test predictions\n",
        "        with torch.no_grad():\n",
        "            for x in test_loader:\n",
        "                # x: (batch size, 30)\n",
        "                x = x.float().to(self.device)\n",
        "                _x = self.model(x)\n",
        "                diff = cos(x, _x).cpu().tolist()\n",
        "                batch_pred = np.where(np.array(diff) < self.thr, 0, 1).tolist()\n",
        "                pred += batch_pred\n",
        "                pred_A += diff  # add test predictions to list\n",
        "        return pred, pred_A\n"
      ],
      "metadata": {
        "id": "t6tu7nWMR7DY"
      },
      "id": "t6tu7nWMR7DY",
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.DataParallel(AutoEncoder())\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
        "\n",
        "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
        "trainer.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Lqy5uR_Px3",
        "outputId": "9d0cb70d-bca4-44a3-a959-e84804c3de0b"
      },
      "id": "D_Lqy5uR_Px3",
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [0] Train loss : [0.5294157692364284] Val Score : [0.009328527964631291])\n",
            "Epoch : [1] Train loss : [0.3421591946056911] Val Score : [0.19926182594361966])\n",
            "Epoch : [2] Train loss : [0.2526163841996874] Val Score : [0.3622378080337761])\n",
            "Epoch : [3] Train loss : [0.20023635029792786] Val Score : [0.43067686434521035])\n",
            "Epoch : [4] Train loss : [0.16755458073956625] Val Score : [0.468002072256841])\n",
            "Epoch : [5] Train loss : [0.14628484206540243] Val Score : [0.48657508769301583])\n",
            "Epoch : [6] Train loss : [0.13157634224210465] Val Score : [0.49477559776715424])\n",
            "Epoch : [7] Train loss : [0.1223030611872673] Val Score : [0.498118180580976])\n",
            "Epoch : [8] Train loss : [0.11527829191514424] Val Score : [0.5012190092285378])\n",
            "Epoch : [9] Train loss : [0.1108942979148456] Val Score : [0.5035856909160894])\n",
            "Epoch : [10] Train loss : [0.10588656472308296] Val Score : [0.5060319599789475])\n",
            "Epoch : [11] Train loss : [0.10174568210329328] Val Score : [0.5080426247561235])\n",
            "Epoch : [12] Train loss : [0.09644459400858198] Val Score : [0.5107812529684067])\n",
            "Epoch : [13] Train loss : [0.09431591842855726] Val Score : [0.5127343022005592])\n",
            "Epoch : [14] Train loss : [0.08997676308665957] Val Score : [0.5141408650492858])\n",
            "Epoch : [15] Train loss : [0.09132739369358335] Val Score : [0.5150202792071831])\n",
            "Epoch : [16] Train loss : [0.0878675782254764] Val Score : [0.5158011122631034])\n",
            "Epoch : [17] Train loss : [0.08768291452101298] Val Score : [0.5176235725945239])\n",
            "Epoch : [18] Train loss : [0.08271117721285139] Val Score : [0.5187218979584981])\n",
            "Epoch : [19] Train loss : [0.08224640148026603] Val Score : [0.5207166130109034])\n",
            "Epoch : [20] Train loss : [0.07912943192890712] Val Score : [0.5216751066978502])\n",
            "Epoch : [21] Train loss : [0.07806096225976944] Val Score : [0.5228080095543691])\n",
            "Epoch : [22] Train loss : [0.07713665174586433] Val Score : [0.524761510660486])\n",
            "Epoch : [23] Train loss : [0.07763842280421938] Val Score : [0.5270492955671493])\n",
            "Epoch : [24] Train loss : [0.07623596489429474] Val Score : [0.5283082015144795])\n",
            "Epoch : [25] Train loss : [0.07455928197928838] Val Score : [0.5311011325592798])\n",
            "Epoch : [26] Train loss : [0.0707719943353108] Val Score : [0.5323255107389778])\n",
            "Epoch : [27] Train loss : [0.07167394246373858] Val Score : [0.5363950248576622])\n",
            "Epoch : [28] Train loss : [0.06928065312760216] Val Score : [0.5390746923434317])\n",
            "Epoch : [29] Train loss : [0.07032817282846995] Val Score : [0.5424284878256398])\n",
            "Epoch : [30] Train loss : [0.07184337505272456] Val Score : [0.5455797786442947])\n",
            "Epoch : [31] Train loss : [0.06800124581371035] Val Score : [0.548391907358596])\n",
            "Epoch : [32] Train loss : [0.0666939445904323] Val Score : [0.5486285923988213])\n",
            "Epoch : [33] Train loss : [0.0674287879041263] Val Score : [0.5493497907415081])\n",
            "Epoch : [34] Train loss : [0.06481734290719032] Val Score : [0.5493497907415081])\n",
            "Epoch : [35] Train loss : [0.06177486053534916] Val Score : [0.5505903833593366])\n",
            "Epoch : [36] Train loss : [0.06392890214920044] Val Score : [0.5517504484954248])\n",
            "Epoch : [37] Train loss : [0.0631720248077597] Val Score : [0.5528183459663629])\n",
            "Epoch : [38] Train loss : [0.06288421686206545] Val Score : [0.5530909538953747])\n",
            "Epoch : [39] Train loss : [0.06481709969895226] Val Score : [0.5540635212673894])\n",
            "Epoch : [40] Train loss : [0.06388068624905177] Val Score : [0.5553578945916403])\n",
            "Epoch : [41] Train loss : [0.060441258762563975] Val Score : [0.5576332952351725])\n",
            "Epoch : [42] Train loss : [0.05988344443695886] Val Score : [0.558749553299279])\n",
            "Epoch : [43] Train loss : [0.05955551617911884] Val Score : [0.5609226813318595])\n",
            "Epoch : [44] Train loss : [0.06153532543352672] Val Score : [0.5616191340522515])\n",
            "Epoch : [45] Train loss : [0.059553993599755425] Val Score : [0.5639801983006851])\n",
            "Epoch : [46] Train loss : [0.05870538204908371] Val Score : [0.5740054732156139])\n",
            "Epoch : [47] Train loss : [0.061835246426718574] Val Score : [0.5831787640520336])\n",
            "Epoch : [48] Train loss : [0.057947716542652676] Val Score : [0.6085712294406179])\n",
            "Epoch : [49] Train loss : [0.0565735287964344] Val Score : [0.6703874788469584])\n",
            "Epoch : [50] Train loss : [0.05628716094153268] Val Score : [0.7059866032567539])\n",
            "Epoch : [51] Train loss : [0.05572559418422835] Val Score : [0.7094766927103557])\n",
            "Epoch : [52] Train loss : [0.0578782499900886] Val Score : [0.7094766927103557])\n",
            "Epoch : [53] Train loss : [0.05555093873824392] Val Score : [0.7059866032567539])\n",
            "Epoch : [54] Train loss : [0.05428377219608852] Val Score : [0.7077171795413468])\n",
            "Epoch : [55] Train loss : [0.05314838354076658] Val Score : [0.7077171795413468])\n",
            "Epoch : [56] Train loss : [0.054482780396938324] Val Score : [0.7094766927103557])\n",
            "Epoch : [57] Train loss : [0.05408150117312159] Val Score : [0.7206844679680786])\n",
            "Epoch : [58] Train loss : [0.05332815647125244] Val Score : [0.7059866032567539])\n",
            "Epoch : [59] Train loss : [0.05165917479566166] Val Score : [0.7187349645015549])\n",
            "Epoch : [60] Train loss : [0.054836714906351905] Val Score : [0.7059866032567539])\n",
            "Epoch : [61] Train loss : [0.05520138410585267] Val Score : [0.7130854976190938])\n",
            "Epoch : [62] Train loss : [0.05417731563959803] Val Score : [0.7130854976190938])\n",
            "Epoch : [63] Train loss : [0.051695565560034344] Val Score : [0.7130854976190938])\n",
            "Epoch : [64] Train loss : [0.05249358979719026] Val Score : [0.7130854976190938])\n",
            "Epoch : [65] Train loss : [0.05235710633652551] Val Score : [0.7267446884090669])\n",
            "Epoch : [66] Train loss : [0.05187618572797094] Val Score : [0.7288385690883094])\n",
            "Epoch : [67] Train loss : [0.05407926493457386] Val Score : [0.7206844679680786])\n",
            "Epoch : [68] Train loss : [0.05005108458655221] Val Score : [0.7168192118976862])\n",
            "Epoch : [69] Train loss : [0.049597471952438354] Val Score : [0.7226686263465465])\n",
            "Epoch : [70] Train loss : [0.05006003752350807] Val Score : [0.7376112450647377])\n",
            "Epoch : [71] Train loss : [0.05044869599597795] Val Score : [0.7331432493795871])\n",
            "Epoch : [72] Train loss : [0.05224186182022095] Val Score : [0.7288385690883094])\n",
            "Epoch : [73] Train loss : [0.050594921090773175] Val Score : [0.7267446884090669])\n",
            "Epoch : [74] Train loss : [0.04863211725439344] Val Score : [0.7353562550268086])\n",
            "Epoch : [75] Train loss : [0.04823743658406394] Val Score : [0.7376112450647377])\n",
            "Epoch : [76] Train loss : [0.05222508045179503] Val Score : [0.7376112450647377])\n",
            "Epoch : [77] Train loss : [0.049800689199141095] Val Score : [0.7376112450647377])\n",
            "Epoch : [78] Train loss : [0.05125729101044791] Val Score : [0.7353562550268086])\n",
            "Epoch : [79] Train loss : [0.050626525921480994] Val Score : [0.7267446884090669])\n",
            "Epoch : [80] Train loss : [0.04865197358386857] Val Score : [0.7353562550268086])\n",
            "Epoch : [81] Train loss : [0.05121004900762013] Val Score : [0.74464046996434])\n",
            "Epoch : [82] Train loss : [0.05028996244072914] Val Score : [0.7495600450513867])\n",
            "Epoch : [83] Train loss : [0.04746355754988534] Val Score : [0.7470759905302604])\n",
            "Epoch : [84] Train loss : [0.04777532070875168] Val Score : [0.7470759905302604])\n",
            "Epoch : [85] Train loss : [0.046304398349353244] Val Score : [0.7422520697342344])\n",
            "Epoch : [86] Train loss : [0.04803830278771264] Val Score : [0.7376112450647377])\n",
            "Epoch : [87] Train loss : [0.048465944826602936] Val Score : [0.7288385690883094])\n",
            "Epoch : [88] Train loss : [0.04701299486415727] Val Score : [0.74464046996434])\n",
            "Epoch : [89] Train loss : [0.04667596785085542] Val Score : [0.7422520697342344])\n",
            "Epoch : [90] Train loss : [0.04643044141786439] Val Score : [0.7495600450513867])\n",
            "Epoch : [91] Train loss : [0.04648453901921] Val Score : [0.7495600450513867])\n",
            "Epoch : [92] Train loss : [0.04515083506703377] Val Score : [0.75467969893057])\n",
            "Epoch : [93] Train loss : [0.044666457389082224] Val Score : [0.7495600450513867])\n",
            "Epoch : [94] Train loss : [0.045584019273519516] Val Score : [0.7495600450513867])\n",
            "Epoch : [95] Train loss : [0.04545073477285249] Val Score : [0.74464046996434])\n",
            "Epoch : [96] Train loss : [0.04691706012402262] Val Score : [0.74464046996434])\n",
            "Epoch : [97] Train loss : [0.04526493325829506] Val Score : [0.7495600450513867])\n",
            "Epoch : [98] Train loss : [0.04731192333357675] Val Score : [0.7399094305905288])\n",
            "Epoch : [99] Train loss : [0.046871052256652286] Val Score : [0.752094104263044])\n",
            "Epoch : [100] Train loss : [0.04607638397387096] Val Score : [0.7495600450513867])\n",
            "Epoch : [101] Train loss : [0.045709548251969476] Val Score : [0.7495600450513867])\n",
            "Epoch : [102] Train loss : [0.04315844178199768] Val Score : [0.7495600450513867])\n",
            "Epoch : [103] Train loss : [0.04316730797290802] Val Score : [0.75467969893057])\n",
            "Epoch 00104: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Epoch : [104] Train loss : [0.04050101978438241] Val Score : [0.752094104263044])\n",
            "Epoch : [105] Train loss : [0.03997282204883439] Val Score : [0.75467969893057])\n",
            "Epoch : [106] Train loss : [0.03998217625277383] Val Score : [0.7495600450513867])\n",
            "Epoch : [107] Train loss : [0.037974433707339425] Val Score : [0.75467969893057])\n",
            "Epoch : [108] Train loss : [0.040032025426626205] Val Score : [0.75467969893057])\n",
            "Epoch : [109] Train loss : [0.039718003677470345] Val Score : [0.75467969893057])\n",
            "Epoch : [110] Train loss : [0.03970175715429442] Val Score : [0.7573184229436457])\n",
            "Epoch : [111] Train loss : [0.03747634696108954] Val Score : [0.7495600450513867])\n",
            "Epoch : [112] Train loss : [0.037780956498214176] Val Score : [0.75467969893057])\n",
            "Epoch : [113] Train loss : [0.039877710597855706] Val Score : [0.7573184229436457])\n",
            "Epoch : [114] Train loss : [0.041344167930739265] Val Score : [0.7573184229436457])\n",
            "Epoch : [115] Train loss : [0.037701502442359924] Val Score : [0.7573184229436457])\n",
            "Epoch : [116] Train loss : [0.03757792232292039] Val Score : [0.75467969893057])\n",
            "Epoch : [117] Train loss : [0.04048792964645794] Val Score : [0.7573184229436457])\n",
            "Epoch : [118] Train loss : [0.03827159638915743] Val Score : [0.75467969893057])\n",
            "Epoch : [119] Train loss : [0.03715437224933079] Val Score : [0.752094104263044])\n",
            "Epoch : [120] Train loss : [0.03920022398233414] Val Score : [0.75467969893057])\n",
            "Epoch : [121] Train loss : [0.03857804781624249] Val Score : [0.752094104263044])\n",
            "Epoch 00122: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Epoch : [122] Train loss : [0.03685084357857704] Val Score : [0.7600119366040216])\n",
            "Epoch : [123] Train loss : [0.035868091242653985] Val Score : [0.75467969893057])\n",
            "Epoch : [124] Train loss : [0.03577113896608353] Val Score : [0.7573184229436457])\n",
            "Epoch : [125] Train loss : [0.03426374441811016] Val Score : [0.7600119366040216])\n",
            "Epoch : [126] Train loss : [0.0344877764582634] Val Score : [0.752094104263044])\n",
            "Epoch : [127] Train loss : [0.03354946470686367] Val Score : [0.7573184229436457])\n",
            "Epoch : [128] Train loss : [0.034727336572749276] Val Score : [0.75467969893057])\n",
            "Epoch : [129] Train loss : [0.03418162624750819] Val Score : [0.7495600450513867])\n",
            "Epoch : [130] Train loss : [0.034183114767074585] Val Score : [0.7573184229436457])\n",
            "Epoch : [131] Train loss : [0.035249410197138786] Val Score : [0.75467969893057])\n",
            "Epoch : [132] Train loss : [0.03470924922398159] Val Score : [0.75467969893057])\n",
            "Epoch : [133] Train loss : [0.03309661495898451] Val Score : [0.7573184229436457])\n",
            "Epoch 00134: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Epoch : [134] Train loss : [0.03201360069215298] Val Score : [0.7573184229436457])\n",
            "Epoch : [135] Train loss : [0.031172957803521837] Val Score : [0.7600119366040216])\n",
            "Epoch : [136] Train loss : [0.03091960054423128] Val Score : [0.7573184229436457])\n",
            "Epoch : [137] Train loss : [0.02972007409802505] Val Score : [0.7573184229436457])\n",
            "Epoch : [138] Train loss : [0.030690956062504222] Val Score : [0.7573184229436457])\n",
            "Epoch : [139] Train loss : [0.031855154250349314] Val Score : [0.7573184229436457])\n",
            "Epoch : [140] Train loss : [0.03164491030786719] Val Score : [0.7600119366040216])\n",
            "Epoch : [141] Train loss : [0.031924125871488025] Val Score : [0.7573184229436457])\n",
            "Epoch : [142] Train loss : [0.03441404498049191] Val Score : [0.7600119366040216])\n",
            "Epoch : [143] Train loss : [0.03362951268042837] Val Score : [0.762761970120889])\n",
            "Epoch : [144] Train loss : [0.033521177513258796] Val Score : [0.7600119366040216])\n",
            "Epoch : [145] Train loss : [0.03065210979964052] Val Score : [0.762761970120889])\n",
            "Epoch : [146] Train loss : [0.032198674976825714] Val Score : [0.7600119366040216])\n",
            "Epoch : [147] Train loss : [0.03329809063247272] Val Score : [0.762761970120889])\n",
            "Epoch : [148] Train loss : [0.03137780725955963] Val Score : [0.762761970120889])\n",
            "Epoch : [149] Train loss : [0.032772041857242584] Val Score : [0.762761970120889])\n",
            "Epoch : [150] Train loss : [0.03208057023584843] Val Score : [0.762761970120889])\n",
            "Epoch : [151] Train loss : [0.0318113025277853] Val Score : [0.762761970120889])\n",
            "Epoch : [152] Train loss : [0.03252659072833402] Val Score : [0.762761970120889])\n",
            "Epoch : [153] Train loss : [0.03311511767762048] Val Score : [0.7600119366040216])\n",
            "Epoch : [154] Train loss : [0.031453842031104226] Val Score : [0.762761970120889])\n",
            "Epoch 00155: reducing learning rate of group 0 to 6.2500e-04.\n",
            "Epoch : [155] Train loss : [0.03027511973466192] Val Score : [0.762761970120889])\n",
            "Epoch : [156] Train loss : [0.030091442966035435] Val Score : [0.762761970120889])\n",
            "Epoch : [157] Train loss : [0.030793183350137303] Val Score : [0.7600119366040216])\n",
            "Epoch : [158] Train loss : [0.0310042460582086] Val Score : [0.762761970120889])\n",
            "Epoch : [159] Train loss : [0.03084853451166834] Val Score : [0.762761970120889])\n",
            "Epoch : [160] Train loss : [0.03111099105860506] Val Score : [0.762761970120889])\n",
            "Epoch : [161] Train loss : [0.030335923390729085] Val Score : [0.762761970120889])\n",
            "Epoch : [162] Train loss : [0.03034450566130025] Val Score : [0.7600119366040216])\n",
            "Epoch : [163] Train loss : [0.030055067368916104] Val Score : [0.762761970120889])\n",
            "Epoch : [164] Train loss : [0.02920681158346789] Val Score : [0.762761970120889])\n",
            "Epoch : [165] Train loss : [0.032544023756469996] Val Score : [0.762761970120889])\n",
            "Epoch 00166: reducing learning rate of group 0 to 3.1250e-04.\n",
            "Epoch : [166] Train loss : [0.03107842111161777] Val Score : [0.762761970120889])\n",
            "Epoch : [167] Train loss : [0.029850856001888002] Val Score : [0.762761970120889])\n",
            "Epoch : [168] Train loss : [0.03067707802568163] Val Score : [0.762761970120889])\n",
            "Epoch : [169] Train loss : [0.02911906210439546] Val Score : [0.762761970120889])\n",
            "Epoch : [170] Train loss : [0.030720338225364685] Val Score : [0.762761970120889])\n",
            "Epoch : [171] Train loss : [0.028353726491332054] Val Score : [0.762761970120889])\n",
            "Epoch : [172] Train loss : [0.02922943447317396] Val Score : [0.762761970120889])\n",
            "Epoch : [173] Train loss : [0.029612483722823008] Val Score : [0.762761970120889])\n",
            "Epoch : [174] Train loss : [0.03157788009515831] Val Score : [0.762761970120889])\n",
            "Epoch : [175] Train loss : [0.02951546518930367] Val Score : [0.762761970120889])\n",
            "Epoch : [176] Train loss : [0.03199151983218534] Val Score : [0.762761970120889])\n",
            "Epoch 00177: reducing learning rate of group 0 to 1.5625e-04.\n",
            "Epoch : [177] Train loss : [0.029265514441898892] Val Score : [0.762761970120889])\n",
            "Epoch : [178] Train loss : [0.028678111199821745] Val Score : [0.762761970120889])\n",
            "Epoch : [179] Train loss : [0.029992779982941493] Val Score : [0.762761970120889])\n",
            "Epoch : [180] Train loss : [0.030463574720280513] Val Score : [0.762761970120889])\n",
            "Epoch : [181] Train loss : [0.029503326596958295] Val Score : [0.762761970120889])\n",
            "Epoch : [182] Train loss : [0.030649377831390927] Val Score : [0.762761970120889])\n",
            "Epoch : [183] Train loss : [0.02874854952096939] Val Score : [0.762761970120889])\n",
            "Epoch : [184] Train loss : [0.02933738167796816] Val Score : [0.762761970120889])\n",
            "Epoch : [185] Train loss : [0.03002982959151268] Val Score : [0.762761970120889])\n",
            "Epoch : [186] Train loss : [0.028730180646692003] Val Score : [0.762761970120889])\n",
            "Epoch : [187] Train loss : [0.030471109119909152] Val Score : [0.762761970120889])\n",
            "Epoch 00188: reducing learning rate of group 0 to 7.8125e-05.\n",
            "Epoch : [188] Train loss : [0.028955283175621713] Val Score : [0.762761970120889])\n",
            "Epoch : [189] Train loss : [0.030160176434687207] Val Score : [0.762761970120889])\n",
            "Epoch : [190] Train loss : [0.03075124403195722] Val Score : [0.762761970120889])\n",
            "Epoch : [191] Train loss : [0.0314114261418581] Val Score : [0.762761970120889])\n",
            "Epoch : [192] Train loss : [0.031123100380812372] Val Score : [0.762761970120889])\n",
            "Epoch : [193] Train loss : [0.029566605176244463] Val Score : [0.762761970120889])\n",
            "Epoch : [194] Train loss : [0.02887116266148431] Val Score : [0.762761970120889])\n",
            "Epoch : [195] Train loss : [0.031102029340607778] Val Score : [0.762761970120889])\n",
            "Epoch : [196] Train loss : [0.029172019500817572] Val Score : [0.762761970120889])\n",
            "Epoch : [197] Train loss : [0.028977069737655774] Val Score : [0.762761970120889])\n",
            "Epoch : [198] Train loss : [0.030038083238261088] Val Score : [0.762761970120889])\n",
            "Epoch 00199: reducing learning rate of group 0 to 3.9063e-05.\n",
            "Epoch : [199] Train loss : [0.0288653059729508] Val Score : [0.762761970120889])\n",
            "Epoch : [200] Train loss : [0.030023072713187764] Val Score : [0.762761970120889])\n",
            "Epoch : [201] Train loss : [0.029399906950337545] Val Score : [0.762761970120889])\n",
            "Epoch : [202] Train loss : [0.030669585934707096] Val Score : [0.762761970120889])\n",
            "Epoch : [203] Train loss : [0.03142131891633783] Val Score : [0.762761970120889])\n",
            "Epoch : [204] Train loss : [0.029166948848537037] Val Score : [0.762761970120889])\n",
            "Epoch : [205] Train loss : [0.029697771051100323] Val Score : [0.762761970120889])\n",
            "Epoch : [206] Train loss : [0.029096596741250584] Val Score : [0.762761970120889])\n",
            "Epoch : [207] Train loss : [0.03044846441064562] Val Score : [0.762761970120889])\n",
            "Epoch : [208] Train loss : [0.030091317370533943] Val Score : [0.762761970120889])\n",
            "Epoch : [209] Train loss : [0.029209237013544356] Val Score : [0.762761970120889])\n",
            "Epoch 00210: reducing learning rate of group 0 to 1.9531e-05.\n",
            "Epoch : [210] Train loss : [0.029503632336854935] Val Score : [0.762761970120889])\n",
            "Epoch : [211] Train loss : [0.029719318662370955] Val Score : [0.762761970120889])\n",
            "Epoch : [212] Train loss : [0.02976019254752568] Val Score : [0.762761970120889])\n",
            "Epoch : [213] Train loss : [0.030652039285217012] Val Score : [0.762761970120889])\n",
            "Epoch : [214] Train loss : [0.029072726145386696] Val Score : [0.762761970120889])\n",
            "Epoch : [215] Train loss : [0.030219673312136104] Val Score : [0.762761970120889])\n",
            "Epoch : [216] Train loss : [0.029096926961626326] Val Score : [0.762761970120889])\n",
            "Epoch : [217] Train loss : [0.029139281383582523] Val Score : [0.762761970120889])\n",
            "Epoch : [218] Train loss : [0.02968993250812803] Val Score : [0.762761970120889])\n",
            "Epoch : [219] Train loss : [0.02951943874359131] Val Score : [0.762761970120889])\n",
            "Epoch : [220] Train loss : [0.029160570619361743] Val Score : [0.762761970120889])\n",
            "Epoch 00221: reducing learning rate of group 0 to 9.7656e-06.\n",
            "Epoch : [221] Train loss : [0.030309738857405528] Val Score : [0.762761970120889])\n",
            "Epoch : [222] Train loss : [0.031123533312763487] Val Score : [0.762761970120889])\n",
            "Epoch : [223] Train loss : [0.028751848531620845] Val Score : [0.762761970120889])\n",
            "Epoch : [224] Train loss : [0.028714228155357496] Val Score : [0.762761970120889])\n",
            "Epoch : [225] Train loss : [0.028270121663808823] Val Score : [0.762761970120889])\n",
            "Epoch : [226] Train loss : [0.029241800840411867] Val Score : [0.762761970120889])\n",
            "Epoch : [227] Train loss : [0.029602271371654103] Val Score : [0.762761970120889])\n",
            "Epoch : [228] Train loss : [0.030037939814584597] Val Score : [0.762761970120889])\n",
            "Epoch : [229] Train loss : [0.029297247529029846] Val Score : [0.762761970120889])\n",
            "Epoch : [230] Train loss : [0.02936390335006373] Val Score : [0.762761970120889])\n",
            "Epoch : [231] Train loss : [0.028765512896435603] Val Score : [0.762761970120889])\n",
            "Epoch 00232: reducing learning rate of group 0 to 4.8828e-06.\n",
            "Epoch : [232] Train loss : [0.02893963243280138] Val Score : [0.762761970120889])\n",
            "Epoch : [233] Train loss : [0.029144191316195896] Val Score : [0.762761970120889])\n",
            "Epoch : [234] Train loss : [0.0288141526814018] Val Score : [0.762761970120889])\n",
            "Epoch : [235] Train loss : [0.029362871976835386] Val Score : [0.762761970120889])\n",
            "Epoch : [236] Train loss : [0.028507108134882792] Val Score : [0.762761970120889])\n",
            "Epoch : [237] Train loss : [0.028814581355878284] Val Score : [0.762761970120889])\n",
            "Epoch : [238] Train loss : [0.028324275144508908] Val Score : [0.762761970120889])\n",
            "Epoch : [239] Train loss : [0.030188911727496555] Val Score : [0.762761970120889])\n",
            "Epoch : [240] Train loss : [0.030620414231504713] Val Score : [0.762761970120889])\n",
            "Epoch : [241] Train loss : [0.02879508212208748] Val Score : [0.762761970120889])\n",
            "Epoch : [242] Train loss : [0.029771865478583744] Val Score : [0.762761970120889])\n",
            "Epoch 00243: reducing learning rate of group 0 to 2.4414e-06.\n",
            "Epoch : [243] Train loss : [0.028686327327575003] Val Score : [0.762761970120889])\n",
            "Epoch : [244] Train loss : [0.030240426372204508] Val Score : [0.762761970120889])\n",
            "Epoch : [245] Train loss : [0.03096859503005232] Val Score : [0.762761970120889])\n",
            "Epoch : [246] Train loss : [0.02989857643842697] Val Score : [0.762761970120889])\n",
            "Epoch : [247] Train loss : [0.0300766471773386] Val Score : [0.762761970120889])\n",
            "Epoch : [248] Train loss : [0.029832464243684496] Val Score : [0.762761970120889])\n",
            "Epoch : [249] Train loss : [0.029443140008619854] Val Score : [0.762761970120889])\n",
            "Epoch : [250] Train loss : [0.028474994535957063] Val Score : [0.762761970120889])\n",
            "Epoch : [251] Train loss : [0.030461303357567106] Val Score : [0.762761970120889])\n",
            "Epoch : [252] Train loss : [0.02904621511697769] Val Score : [0.762761970120889])\n",
            "Epoch : [253] Train loss : [0.030109443834849765] Val Score : [0.762761970120889])\n",
            "Epoch 00254: reducing learning rate of group 0 to 1.2207e-06.\n",
            "Epoch : [254] Train loss : [0.02870943902858666] Val Score : [0.762761970120889])\n",
            "Epoch : [255] Train loss : [0.02988761610218457] Val Score : [0.762761970120889])\n",
            "Epoch : [256] Train loss : [0.028814349057418958] Val Score : [0.762761970120889])\n",
            "Epoch : [257] Train loss : [0.029471266216465404] Val Score : [0.762761970120889])\n",
            "Epoch : [258] Train loss : [0.029353455507329533] Val Score : [0.762761970120889])\n",
            "Epoch : [259] Train loss : [0.030269762501120567] Val Score : [0.762761970120889])\n",
            "Epoch : [260] Train loss : [0.028399874855365072] Val Score : [0.762761970120889])\n",
            "Epoch : [261] Train loss : [0.030987444200686047] Val Score : [0.762761970120889])\n",
            "Epoch : [262] Train loss : [0.030026963778904507] Val Score : [0.762761970120889])\n",
            "Epoch : [263] Train loss : [0.02886312348502023] Val Score : [0.762761970120889])\n",
            "Epoch : [264] Train loss : [0.03028873833162444] Val Score : [0.762761970120889])\n",
            "Epoch 00265: reducing learning rate of group 0 to 6.1035e-07.\n",
            "Epoch : [265] Train loss : [0.030617178018604006] Val Score : [0.762761970120889])\n",
            "Epoch : [266] Train loss : [0.030011537351778576] Val Score : [0.762761970120889])\n",
            "Epoch : [267] Train loss : [0.027284856087395122] Val Score : [0.762761970120889])\n",
            "Epoch : [268] Train loss : [0.02935884493802275] Val Score : [0.762761970120889])\n",
            "Epoch : [269] Train loss : [0.030328629538416862] Val Score : [0.762761970120889])\n",
            "Epoch : [270] Train loss : [0.02890849539211818] Val Score : [0.762761970120889])\n",
            "Epoch : [271] Train loss : [0.030570106846945628] Val Score : [0.762761970120889])\n",
            "Epoch : [272] Train loss : [0.028050062113574574] Val Score : [0.762761970120889])\n",
            "Epoch : [273] Train loss : [0.02993722925228732] Val Score : [0.762761970120889])\n",
            "Epoch : [274] Train loss : [0.03113036123769624] Val Score : [0.762761970120889])\n",
            "Epoch : [275] Train loss : [0.03063424357346126] Val Score : [0.762761970120889])\n",
            "Epoch 00276: reducing learning rate of group 0 to 3.0518e-07.\n",
            "Epoch : [276] Train loss : [0.02979610940175397] Val Score : [0.762761970120889])\n",
            "Epoch : [277] Train loss : [0.030228917353919575] Val Score : [0.762761970120889])\n",
            "Epoch : [278] Train loss : [0.02933020783322198] Val Score : [0.762761970120889])\n",
            "Epoch : [279] Train loss : [0.029295907222798893] Val Score : [0.762761970120889])\n",
            "Epoch : [280] Train loss : [0.02846048931990351] Val Score : [0.762761970120889])\n",
            "Epoch : [281] Train loss : [0.030522703060082028] Val Score : [0.762761970120889])\n",
            "Epoch : [282] Train loss : [0.02947362246257918] Val Score : [0.762761970120889])\n",
            "Epoch : [283] Train loss : [0.030232981645635197] Val Score : [0.762761970120889])\n",
            "Epoch : [284] Train loss : [0.029930369396294867] Val Score : [0.762761970120889])\n",
            "Epoch : [285] Train loss : [0.029919972643256187] Val Score : [0.762761970120889])\n",
            "Epoch : [286] Train loss : [0.029559341658438956] Val Score : [0.762761970120889])\n",
            "Epoch 00287: reducing learning rate of group 0 to 1.5259e-07.\n",
            "Epoch : [287] Train loss : [0.0314100586942264] Val Score : [0.762761970120889])\n",
            "Epoch : [288] Train loss : [0.029697106618966376] Val Score : [0.762761970120889])\n",
            "Epoch : [289] Train loss : [0.028892340936831067] Val Score : [0.762761970120889])\n",
            "Epoch : [290] Train loss : [0.028323314285704067] Val Score : [0.762761970120889])\n",
            "Epoch : [291] Train loss : [0.029313085866825923] Val Score : [0.762761970120889])\n",
            "Epoch : [292] Train loss : [0.03129523620009422] Val Score : [0.762761970120889])\n",
            "Epoch : [293] Train loss : [0.03048723324068955] Val Score : [0.762761970120889])\n",
            "Epoch : [294] Train loss : [0.030004747211933136] Val Score : [0.762761970120889])\n",
            "Epoch : [295] Train loss : [0.02879350126854011] Val Score : [0.762761970120889])\n",
            "Epoch : [296] Train loss : [0.028276570407407626] Val Score : [0.762761970120889])\n",
            "Epoch : [297] Train loss : [0.027831170707941055] Val Score : [0.762761970120889])\n",
            "Epoch 00298: reducing learning rate of group 0 to 7.6294e-08.\n",
            "Epoch : [298] Train loss : [0.029917554131575992] Val Score : [0.762761970120889])\n",
            "Epoch : [299] Train loss : [0.028912135532924106] Val Score : [0.762761970120889])\n",
            "Epoch : [300] Train loss : [0.03052928830896105] Val Score : [0.762761970120889])\n",
            "Epoch : [301] Train loss : [0.03003729640373162] Val Score : [0.762761970120889])\n",
            "Epoch : [302] Train loss : [0.0291026985006673] Val Score : [0.762761970120889])\n",
            "Epoch : [303] Train loss : [0.029918352674160684] Val Score : [0.762761970120889])\n",
            "Epoch : [304] Train loss : [0.029777766338416507] Val Score : [0.762761970120889])\n",
            "Epoch : [305] Train loss : [0.02997700497508049] Val Score : [0.762761970120889])\n",
            "Epoch : [306] Train loss : [0.029029690527490208] Val Score : [0.762761970120889])\n",
            "Epoch : [307] Train loss : [0.029390128329396248] Val Score : [0.762761970120889])\n",
            "Epoch : [308] Train loss : [0.030146654163088118] Val Score : [0.762761970120889])\n",
            "Epoch 00309: reducing learning rate of group 0 to 3.8147e-08.\n",
            "Epoch : [309] Train loss : [0.030019056318061694] Val Score : [0.762761970120889])\n",
            "Epoch : [310] Train loss : [0.029349563111151968] Val Score : [0.762761970120889])\n",
            "Epoch : [311] Train loss : [0.02930711316210883] Val Score : [0.762761970120889])\n",
            "Epoch : [312] Train loss : [0.028961061100874628] Val Score : [0.762761970120889])\n",
            "Epoch : [313] Train loss : [0.030955244920083454] Val Score : [0.762761970120889])\n",
            "Epoch : [314] Train loss : [0.030725291264908656] Val Score : [0.762761970120889])\n",
            "Epoch : [315] Train loss : [0.028272186539002826] Val Score : [0.762761970120889])\n",
            "Epoch : [316] Train loss : [0.02888945969087737] Val Score : [0.762761970120889])\n",
            "Epoch : [317] Train loss : [0.029254077534590448] Val Score : [0.762761970120889])\n",
            "Epoch : [318] Train loss : [0.029221095410840853] Val Score : [0.762761970120889])\n",
            "Epoch : [319] Train loss : [0.02849337857748781] Val Score : [0.762761970120889])\n",
            "Epoch 00320: reducing learning rate of group 0 to 1.9073e-08.\n",
            "Epoch : [320] Train loss : [0.029566766694188118] Val Score : [0.762761970120889])\n",
            "Epoch : [321] Train loss : [0.028957647936684743] Val Score : [0.762761970120889])\n",
            "Epoch : [322] Train loss : [0.02935397412095751] Val Score : [0.762761970120889])\n",
            "Epoch : [323] Train loss : [0.028804768941232135] Val Score : [0.762761970120889])\n",
            "Epoch : [324] Train loss : [0.028981788882187436] Val Score : [0.762761970120889])\n",
            "Epoch : [325] Train loss : [0.029531578134213175] Val Score : [0.762761970120889])\n",
            "Epoch : [326] Train loss : [0.02862760637487684] Val Score : [0.762761970120889])\n",
            "Epoch : [327] Train loss : [0.030009354863848006] Val Score : [0.762761970120889])\n",
            "Epoch : [328] Train loss : [0.028345690774066106] Val Score : [0.762761970120889])\n",
            "Epoch : [329] Train loss : [0.02951872455222266] Val Score : [0.762761970120889])\n",
            "Epoch : [330] Train loss : [0.028902739552514895] Val Score : [0.762761970120889])\n",
            "Epoch : [331] Train loss : [0.02871349666799818] Val Score : [0.762761970120889])\n",
            "Epoch : [332] Train loss : [0.029594028102500097] Val Score : [0.762761970120889])\n",
            "Epoch : [333] Train loss : [0.02922064997255802] Val Score : [0.762761970120889])\n",
            "Epoch : [334] Train loss : [0.03108252664761884] Val Score : [0.762761970120889])\n",
            "Epoch : [335] Train loss : [0.030456834339669774] Val Score : [0.762761970120889])\n",
            "Epoch : [336] Train loss : [0.02919625278030123] Val Score : [0.762761970120889])\n",
            "Epoch : [337] Train loss : [0.031442061332719665] Val Score : [0.762761970120889])\n",
            "Epoch : [338] Train loss : [0.028616717883518765] Val Score : [0.762761970120889])\n",
            "Epoch : [339] Train loss : [0.028903082545314516] Val Score : [0.762761970120889])\n",
            "Epoch : [340] Train loss : [0.030186599120497704] Val Score : [0.762761970120889])\n",
            "Epoch : [341] Train loss : [0.02888088220996516] Val Score : [0.762761970120889])\n",
            "Epoch : [342] Train loss : [0.03042831750852721] Val Score : [0.762761970120889])\n",
            "Epoch : [343] Train loss : [0.030249438115528653] Val Score : [0.762761970120889])\n",
            "Epoch : [344] Train loss : [0.02888085533465658] Val Score : [0.762761970120889])\n",
            "Epoch : [345] Train loss : [0.029033327475190163] Val Score : [0.762761970120889])\n",
            "Epoch : [346] Train loss : [0.029844671221716062] Val Score : [0.762761970120889])\n",
            "Epoch : [347] Train loss : [0.029452506186706678] Val Score : [0.762761970120889])\n",
            "Epoch : [348] Train loss : [0.03159934415348938] Val Score : [0.762761970120889])\n",
            "Epoch : [349] Train loss : [0.029177216812968254] Val Score : [0.762761970120889])\n",
            "Epoch : [350] Train loss : [0.028626447543501854] Val Score : [0.762761970120889])\n",
            "Epoch : [351] Train loss : [0.030432188351239477] Val Score : [0.762761970120889])\n",
            "Epoch : [352] Train loss : [0.03037928231060505] Val Score : [0.762761970120889])\n",
            "Epoch : [353] Train loss : [0.030970208879028047] Val Score : [0.762761970120889])\n",
            "Epoch : [354] Train loss : [0.030501193766083037] Val Score : [0.762761970120889])\n",
            "Epoch : [355] Train loss : [0.029587760035480772] Val Score : [0.762761970120889])\n",
            "Epoch : [356] Train loss : [0.028510910059724535] Val Score : [0.762761970120889])\n",
            "Epoch : [357] Train loss : [0.030760620587638447] Val Score : [0.762761970120889])\n",
            "Epoch : [358] Train loss : [0.029473642419491495] Val Score : [0.762761970120889])\n",
            "Epoch : [359] Train loss : [0.02899360204381602] Val Score : [0.762761970120889])\n",
            "Epoch : [360] Train loss : [0.02967933937907219] Val Score : [0.762761970120889])\n",
            "Epoch : [361] Train loss : [0.029131423415882245] Val Score : [0.762761970120889])\n",
            "Epoch : [362] Train loss : [0.030147805011698177] Val Score : [0.762761970120889])\n",
            "Epoch : [363] Train loss : [0.029220243117638996] Val Score : [0.762761970120889])\n",
            "Epoch : [364] Train loss : [0.029802200251391957] Val Score : [0.762761970120889])\n",
            "Epoch : [365] Train loss : [0.02958202122577599] Val Score : [0.762761970120889])\n",
            "Epoch : [366] Train loss : [0.02883718375648771] Val Score : [0.762761970120889])\n",
            "Epoch : [367] Train loss : [0.02919093386403152] Val Score : [0.762761970120889])\n",
            "Epoch : [368] Train loss : [0.03070975174861295] Val Score : [0.762761970120889])\n",
            "Epoch : [369] Train loss : [0.030361461054001535] Val Score : [0.762761970120889])\n",
            "Epoch : [370] Train loss : [0.029138668041144098] Val Score : [0.762761970120889])\n",
            "Epoch : [371] Train loss : [0.030246008187532425] Val Score : [0.762761970120889])\n",
            "Epoch : [372] Train loss : [0.030173944043261663] Val Score : [0.762761970120889])\n",
            "Epoch : [373] Train loss : [0.03102576227060386] Val Score : [0.762761970120889])\n",
            "Epoch : [374] Train loss : [0.028740630618163517] Val Score : [0.762761970120889])\n",
            "Epoch : [375] Train loss : [0.030906974471041133] Val Score : [0.762761970120889])\n",
            "Epoch : [376] Train loss : [0.02939390843468053] Val Score : [0.762761970120889])\n",
            "Epoch : [377] Train loss : [0.029117827702845846] Val Score : [0.762761970120889])\n",
            "Epoch : [378] Train loss : [0.02840947306581906] Val Score : [0.762761970120889])\n",
            "Epoch : [379] Train loss : [0.030859272394861494] Val Score : [0.762761970120889])\n",
            "Epoch : [380] Train loss : [0.028971991102610315] Val Score : [0.762761970120889])\n",
            "Epoch : [381] Train loss : [0.029078811140997068] Val Score : [0.762761970120889])\n",
            "Epoch : [382] Train loss : [0.029469420069030354] Val Score : [0.762761970120889])\n",
            "Epoch : [383] Train loss : [0.028702662459441593] Val Score : [0.762761970120889])\n",
            "Epoch : [384] Train loss : [0.030617140233516693] Val Score : [0.762761970120889])\n",
            "Epoch : [385] Train loss : [0.029316579390849386] Val Score : [0.762761970120889])\n",
            "Epoch : [386] Train loss : [0.029102256521582603] Val Score : [0.762761970120889])\n",
            "Epoch : [387] Train loss : [0.03201528558773654] Val Score : [0.762761970120889])\n",
            "Epoch : [388] Train loss : [0.02937925766621317] Val Score : [0.762761970120889])\n",
            "Epoch : [389] Train loss : [0.03052334806748799] Val Score : [0.762761970120889])\n",
            "Epoch : [390] Train loss : [0.029128873454672948] Val Score : [0.762761970120889])\n",
            "Epoch : [391] Train loss : [0.03061361690717084] Val Score : [0.762761970120889])\n",
            "Epoch : [392] Train loss : [0.030217815722737993] Val Score : [0.762761970120889])\n",
            "Epoch : [393] Train loss : [0.028352237439581325] Val Score : [0.762761970120889])\n",
            "Epoch : [394] Train loss : [0.030980399144547328] Val Score : [0.762761970120889])\n",
            "Epoch : [395] Train loss : [0.029360157836760794] Val Score : [0.762761970120889])\n",
            "Epoch : [396] Train loss : [0.029148191213607788] Val Score : [0.762761970120889])\n",
            "Epoch : [397] Train loss : [0.02878630001630102] Val Score : [0.762761970120889])\n",
            "Epoch : [398] Train loss : [0.02924899703689984] Val Score : [0.762761970120889])\n",
            "Epoch : [399] Train loss : [0.02857402472623757] Val Score : [0.762761970120889])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder()\n",
        "model.load_state_dict(torch.load('./drive/MyDrive/신용카드 사기 데이콘/best_model.pth'))\n",
        "model = nn.DataParallel(model)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pVdkEiVHnOw",
        "outputId": "85dbc03d-7417-487c-e89d-624b85cc7141"
      },
      "id": "2pVdkEiVHnOw",
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): AutoEncoder(\n",
              "    (Encoder): Sequential(\n",
              "      (0): Linear(in_features=30, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): GELU(approximate='none')\n",
              "    )\n",
              "    (Decoder): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Linear(in_features=64, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dff = valid_df.drop(columns=['Class'])"
      ],
      "metadata": {
        "id": "g5hud8pSSvof"
      },
      "id": "g5hud8pSSvof",
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "va_dataset = MyDataset(valid_dff, False)\n",
        "va_loader = DataLoader(va_dataset, batch_size=BS, shuffle=False, num_workers=6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjuIctwySR7z",
        "outputId": "0294bfde-2fd1-49a5-a976-97d06dbdcd8c"
      },
      "id": "tjuIctwySR7z",
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, thr, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for x in iter(test_loader):\n",
        "            x = x.float().to(device)\n",
        "            \n",
        "            _x = model(x)\n",
        "            \n",
        "            diff = cos(x, _x).cpu().tolist()\n",
        "            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "            pred += batch_pred\n",
        "    return pred"
      ],
      "metadata": {
        "id": "8Bv2B8uWOuFI"
      },
      "id": "8Bv2B8uWOuFI",
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_A = prediction(model, 0.95, va_loader, device)"
      ],
      "metadata": {
        "id": "k5iRJupCSLYK"
      },
      "id": "k5iRJupCSLYK",
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-r0TOr_TOuE",
        "outputId": "2706ae2c-9c3e-41f1-cf20-01b0bcd9d77b"
      },
      "id": "i-r0TOr_TOuE",
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_A = np.array(val_pred_A).reshape(-1)"
      ],
      "metadata": {
        "id": "itGjZpKAWJ9R"
      },
      "id": "itGjZpKAWJ9R",
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYXh3JVeWgOQ",
        "outputId": "056178df-1890-41c3-81a7-aac4c578fd49"
      },
      "id": "VYXh3JVeWgOQ",
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/test.csv')\n",
        "test = test.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "gfBzhegwagdX"
      },
      "id": "gfBzhegwagdX",
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyDataset(test, False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=6)"
      ],
      "metadata": {
        "id": "rH_qYixE5CLv"
      },
      "id": "rH_qYixE5CLv",
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, thr, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for x in iter(test_loader):\n",
        "            x = x.float().to(device)\n",
        "            _x = model(x)\n",
        "            \n",
        "            diff = cos(x, _x).cpu().tolist()\n",
        "            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "            pred += batch_pred\n",
        "    return pred"
      ],
      "metadata": {
        "id": "KkrViUNu5C4j"
      },
      "id": "KkrViUNu5C4j",
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = prediction(model, 0.95, test_loader, device)"
      ],
      "metadata": {
        "id": "yQZ2Jp7f5GZj"
      },
      "id": "yQZ2Jp7f5GZj",
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km7ED4Pr9Q5k",
        "outputId": "48e8c049-63fc-4875-e864-67f9f8c53d24"
      },
      "id": "Km7ED4Pr9Q5k",
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_A = np.array(preds).reshape(-1)"
      ],
      "metadata": {
        "id": "T1p-VjtB9V7L"
      },
      "id": "T1p-VjtB9V7L",
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tFws-md9Zje",
        "outputId": "aea776d6-6a68-4830-9190-37b350c50c31"
      },
      "id": "2tFws-md9Zje",
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE"
      ],
      "metadata": {
        "id": "8eYTXvrjhIPn"
      },
      "id": "8eYTXvrjhIPn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "EIO HardVoting"
      ],
      "metadata": {
        "id": "bb7qmNiCMNDD"
      },
      "id": "bb7qmNiCMNDD"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "ensemble_pred = (val_pred_O + val_pred_I + val_pred_E) >= 2\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (hard voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRdrDTJb4Dqu",
        "outputId": "b20e8d93-37be-4613-d295-dc4abc2bcf99"
      },
      "id": "dRdrDTJb4Dqu",
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (hard voting): 0.9137051774467988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "test_ensemble_pred = (test_pred_O + test_pred_I + test_pred_E) >= 2\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EIO_hard.csv', index=False)"
      ],
      "metadata": {
        "id": "ctnM1AQ_DSk2"
      },
      "id": "ctnM1AQ_DSk2",
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EIA HardVoting"
      ],
      "metadata": {
        "id": "tjvTzuiQWr6K"
      },
      "id": "tjvTzuiQWr6K"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "ensemble_pred = (val_pred_A + val_pred_I + val_pred_E) >= 2\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (hard voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swQzKq6CWxGu",
        "outputId": "f49757f3-aedd-41fe-b729-92e1529cd28b"
      },
      "id": "swQzKq6CWxGu",
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (hard voting): 0.9236496787663914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "test_ensemble_pred = (test_pred_A + test_pred_I + test_pred_E) >= 2\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EIA_hard.csv', index=False)"
      ],
      "metadata": {
        "id": "DeOiiTRABKWv"
      },
      "id": "DeOiiTRABKWv",
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EOA HardVoting"
      ],
      "metadata": {
        "id": "pGVqC5fzWzLR"
      },
      "id": "pGVqC5fzWzLR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "ensemble_pred = (val_pred_A + val_pred_O + val_pred_E) >= 2\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (hard voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_34BUj8W184",
        "outputId": "a81f5dd2-e755-499b-8339-60939eea5ea0"
      },
      "id": "0_34BUj8W184",
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (hard voting): 0.9236496787663914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "test_ensemble_pred = (test_pred_O + test_pred_A + test_pred_E) >= 2\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EOA_hard.csv', index=False)"
      ],
      "metadata": {
        "id": "eh7luaWtBVSF"
      },
      "id": "eh7luaWtBVSF",
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IOA HardVoting"
      ],
      "metadata": {
        "id": "zhyjAo6mW4yY"
      },
      "id": "zhyjAo6mW4yY"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "ensemble_pred = (val_pred_A + val_pred_O + val_pred_I) >= 2\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (hard voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz8BFW2BW8KA",
        "outputId": "dccf3c1a-d37a-4ad7-8d26-bdfabd2e40dc"
      },
      "id": "Xz8BFW2BW8KA",
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (hard voting): 0.9137051774467988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hard voting ensemble\n",
        "test_ensemble_pred = (test_pred_O + test_pred_A + test_pred_I) >= 2\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/IOA_hard.csv', index=False)"
      ],
      "metadata": {
        "id": "igNFu8ubBcKT"
      },
      "id": "igNFu8ubBcKT",
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE (Soft Voting)"
      ],
      "metadata": {
        "id": "9fTxvQBU42B9"
      },
      "id": "9fTxvQBU42B9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "EIO SoftVoting"
      ],
      "metadata": {
        "id": "ZCY-gpftXEDQ"
      },
      "id": "ZCY-gpftXEDQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "ensemble_pred = np.round((val_pred_O + val_pred_I + val_pred_E) / 3)\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (soft voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0g6xT3w41Z-",
        "outputId": "f0331ebb-7e81-4fa2-a7b7-c255d8eb0ae5"
      },
      "id": "d0g6xT3w41Z-",
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (soft voting): 0.9137051774467988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "test_ensemble_pred = np.round((test_pred_O + test_pred_E + test_pred_I) / 3)\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EIO_soft.csv', index=False)"
      ],
      "metadata": {
        "id": "13HFH1BLBlJj"
      },
      "id": "13HFH1BLBlJj",
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EIA SoftVoting"
      ],
      "metadata": {
        "id": "UxN3v32zXHh4"
      },
      "id": "UxN3v32zXHh4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "ensemble_pred = np.round((val_pred_A + val_pred_I + val_pred_E) / 3)\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (soft voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_pnJGLKXL_B",
        "outputId": "27adb854-6bb7-4499-d74a-8023d1d3cab2"
      },
      "id": "k_pnJGLKXL_B",
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (soft voting): 0.9236496787663914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "test_ensemble_pred = np.round((test_pred_A + test_pred_E + test_pred_I) / 3)\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EIA_soft.csv', index=False)"
      ],
      "metadata": {
        "id": "zgKAVDFgBxQB"
      },
      "id": "zgKAVDFgBxQB",
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EOA SoftVoting"
      ],
      "metadata": {
        "id": "a_83PnGdXO83"
      },
      "id": "a_83PnGdXO83"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "ensemble_pred = np.round((val_pred_A + val_pred_O + val_pred_E) / 3)\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (soft voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf-ytqGlXSMr",
        "outputId": "27e30fb8-378d-4838-f49b-92e2f8ab018d"
      },
      "id": "Hf-ytqGlXSMr",
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (soft voting): 0.9236496787663914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "test_ensemble_pred = np.round((test_pred_A + test_pred_E + test_pred_O) / 3)\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/EOA_soft.csv', index=False)"
      ],
      "metadata": {
        "id": "4s2oodR0B1Br"
      },
      "id": "4s2oodR0B1Br",
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IOA Soft Voting"
      ],
      "metadata": {
        "id": "ZCdsDj1CXXNP"
      },
      "id": "ZCdsDj1CXXNP"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "ensemble_pred = np.round((val_pred_A + val_pred_O + val_pred_I) / 3)\n",
        "ensemble_score = f1_score(val_y, ensemble_pred, average='macro')\n",
        "print(f'Ensemble F1 Score (soft voting): {ensemble_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_bNBB7HXZDe",
        "outputId": "f289b0e1-293e-4e05-b540-137d399fc21e"
      },
      "id": "h_bNBB7HXZDe",
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1 Score (soft voting): 0.9137051774467988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Soft voting ensemble\n",
        "test_ensemble_pred = np.round((test_pred_A + test_pred_O + test_pred_I) / 3)\n",
        "ensemble_pred_int = test_ensemble_pred.astype(int)\n",
        "submit = pd.read_csv('./drive/MyDrive/신용카드 사기 데이콘/open/sample_submission.csv')\n",
        "submit['Class'] = ensemble_pred_int\n",
        "submit.to_csv('./drive/MyDrive/신용카드 사기 데이콘/open/IOA_soft.csv', index=False)"
      ],
      "metadata": {
        "id": "C0WmJHM2B3yh"
      },
      "id": "C0WmJHM2B3yh",
      "execution_count": 302,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}