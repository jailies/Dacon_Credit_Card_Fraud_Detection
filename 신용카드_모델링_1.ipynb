{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJu97PWEzUr5"
   },
   "source": [
    "## Import & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u8Wydav5x9tN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GnIphsrtyOfg"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-x0m-FjpyQsC"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('open/train.csv')\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "val_df = pd.read_csv('open/val.csv')\n",
    "val_df = val_df.drop(columns=['ID'])\n",
    "\n",
    "new_train_df = train_df[['V2','V3','V4','V7','V9','V10','V11','V12','V14','V16','V17','V18']]\n",
    "new_val_df = val_df[['V2','V3','V4','V7','V9','V10','V11','V12','V14','V16','V17','V18','Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbq_CpH8JHF-"
   },
   "source": [
    "## Method 1. GANomaly와 EllipticEnvelope를 이용한 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL4Iotloz_Hp"
   },
   "source": [
    "## GANomaly\n",
    "\n",
    "*   GANomaly는 semi-supervised 이상치 탐지에 사용되는 모델입니다\n",
    "\n",
    "*   기존의 AnoGAN에 비해 Encoder 부분이 추가 됨으로써 latent vector를 두 번 생성하여 loss를 한 번 더 계산해주어 정확도를 높인다는 점이 장점이라 할 수 있습니다다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYKQNLcZzP0z",
    "outputId": "baa36494-9738-4eda-ef88-6f12badb29b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-la-0m4qyZdK"
   },
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "batch_size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NsSOLAKLyaDd"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gD_Y2Zj9y5v4"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8JxtYSTcy8JG"
   },
   "outputs": [],
   "source": [
    "class GANnomaly(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "        \n",
    "        self.models = [self.encoder, self.decoder, self.discriminator]   \n",
    "                \n",
    "        self.params = None\n",
    "        for idx_m, model in enumerate(self.models):\n",
    "            if (self.params is None):\n",
    "                self.params = list(model.parameters())\n",
    "            else:\n",
    "                self.params = self.params + list(model.parameters())\n",
    "                \n",
    "        self.optimizer = optim.Adam(self.params, lr=0.0001)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(30,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        z_code = self.encoder(x)\n",
    "\n",
    "        return z_code\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64,30),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_hat = self.decoder(x)\n",
    "\n",
    "        return x_hat\n",
    "    \n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.dis_dense = nn.ModuleList([\n",
    "            nn.Linear(30, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid(),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        featurebank = []\n",
    "        \n",
    "        for idx, layer in enumerate(self.dis_dense):\n",
    "            x = layer(x)\n",
    "            if(\"torch.nn.modules.activation\" in str(type(layer))):\n",
    "                featurebank.append(x)\n",
    "        disc_score = x\n",
    "\n",
    "        return disc_score, featurebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uSRdJRPm0aw9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_enc(z_code, z_code_hat):\n",
    "\n",
    "    l_enc = torch.sum((z_code - z_code_hat)**2, dim=(1))\n",
    "\n",
    "    return l_enc\n",
    "\n",
    "def loss_rec(x, x_hat):\n",
    "    l_con = torch.sum(torch.abs(x - x_hat), dim=(1))\n",
    "\n",
    "    return l_con\n",
    "\n",
    "def loss_adv(dis_x, dis_x_hat, features_real, features_fake):\n",
    "\n",
    "    l_adv = torch.sum((dis_x - dis_x_hat)**2, dim=(1))\n",
    "\n",
    "\n",
    "    for fidx, _ in enumerate(features_real):\n",
    "        \n",
    "        l_adv += torch.sum((features_real[fidx] - features_fake[fidx])**2, dim=(1))\n",
    "            \n",
    "    return l_adv\n",
    "\n",
    "\n",
    "def loss_gan(z_code, z_code_hat, x, x_hat,\n",
    "    dis_x, dis_x_hat, features_real, features_fake,\n",
    "    w_enc=1, w_con=50, w_adv=1):\n",
    "    \n",
    "\n",
    "    z_code, z_code_hat, x, x_hat = z_code, z_code_hat, x, x_hat\n",
    "\n",
    "    for fidx, _ in enumerate(features_real):\n",
    "        features_real[fidx] = features_real[fidx]\n",
    "        features_fake[fidx] = features_fake[fidx]\n",
    "        \n",
    "    l_enc = loss_enc(z_code, z_code_hat)\n",
    "    l_con = loss_rec(x, x_hat)\n",
    "    l_adv = loss_adv(dis_x, dis_x_hat, features_real, features_fake)\n",
    "\n",
    "\n",
    "    l_tot = torch.mean((w_enc * l_enc) + (w_con * l_con) + (w_adv * l_adv))\n",
    "    \n",
    "    l_enc = torch.mean(l_enc)\n",
    "    l_con = torch.mean(l_con)\n",
    "    l_adv = torch.mean(l_adv)\n",
    "\n",
    "\n",
    "    return l_tot, l_enc, l_con, l_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlDX_9Mjy-mr",
    "outputId": "f2c3bf78-c4f3-48c7-a730-e117548ca0b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train loss : [1463.4795968191963] Val Score : [0.0010529271374420891])\n",
      "Epoch : [2] Train loss : [1081.2173723493304] Val Score : [0.0010529271374420891])\n",
      "Epoch : [3] Train loss : [1047.4806431361608] Val Score : [0.0010529271374420891])\n",
      "Epoch : [4] Train loss : [1020.5997924804688] Val Score : [0.0010529271374420891])\n",
      "Epoch : [5] Train loss : [997.6226545061384] Val Score : [0.0010529271374420891])\n",
      "Epoch : [6] Train loss : [976.8873291015625] Val Score : [0.0010529271374420891])\n",
      "Epoch : [7] Train loss : [957.3501935686384] Val Score : [0.0010529271374420891])\n",
      "Epoch : [8] Train loss : [938.3730119977679] Val Score : [0.0010529271374420891])\n",
      "Epoch : [9] Train loss : [919.4818115234375] Val Score : [0.0010529271374420891])\n",
      "Epoch : [10] Train loss : [900.511492047991] Val Score : [0.0010529271374420891])\n",
      "Epoch : [11] Train loss : [881.2325352260044] Val Score : [0.0010529271374420891])\n",
      "Epoch : [12] Train loss : [861.3802228655134] Val Score : [0.0010529271374420891])\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch : [13] Train loss : [845.6749093191964] Val Score : [0.0010881344945152598])\n",
      "Epoch : [14] Train loss : [835.5118669782366] Val Score : [0.0011937417393830074])\n",
      "Epoch : [15] Train loss : [825.2803431919643] Val Score : [0.0012993267502086495])\n",
      "Epoch : [16] Train loss : [814.9385288783482] Val Score : [0.0016862816792256323])\n",
      "Epoch : [17] Train loss : [804.620832170759] Val Score : [0.0019323702948910873])\n",
      "Epoch : [18] Train loss : [794.2438005719866] Val Score : [0.002108073923125571])\n",
      "Epoch : [19] Train loss : [783.8711199079241] Val Score : [0.0023188369787300253])\n",
      "Epoch : [20] Train loss : [773.6335536411831] Val Score : [0.0025997165177361684])\n",
      "Epoch : [21] Train loss : [763.4937220982143] Val Score : [0.0029505945542486044])\n",
      "Epoch : [22] Train loss : [753.6007167271206] Val Score : [0.00337132383931058])\n",
      "Epoch : [23] Train loss : [743.818830217634] Val Score : [0.004141743266311977])\n",
      "Epoch : [24] Train loss : [734.133318219866] Val Score : [0.0051904058092205226])\n",
      "Epoch : [25] Train loss : [724.5258265904018] Val Score : [0.006585206451410451])\n",
      "Epoch : [26] Train loss : [714.9430454799107] Val Score : [0.008531397385520587])\n",
      "Epoch : [27] Train loss : [705.4180210658482] Val Score : [0.010677271403677097])\n",
      "Epoch : [28] Train loss : [695.8288835797991] Val Score : [0.012435505412558601])\n",
      "Epoch : [29] Train loss : [686.2465907505581] Val Score : [0.014221819830498245])\n",
      "Epoch : [30] Train loss : [676.6456821986607] Val Score : [0.016309121045319903])\n",
      "Epoch : [31] Train loss : [667.042724609375] Val Score : [0.018625600254148583])\n",
      "Epoch : [32] Train loss : [657.4588187081473] Val Score : [0.021370801188381985])\n",
      "Epoch : [33] Train loss : [647.7819562639509] Val Score : [0.025007375031997996])\n",
      "Epoch : [34] Train loss : [637.9308994838169] Val Score : [0.03041207270879004])\n",
      "Epoch : [35] Train loss : [628.2167619977679] Val Score : [0.03572464353912363])\n",
      "Epoch : [36] Train loss : [618.3200945172991] Val Score : [0.04237345749227591])\n",
      "Epoch : [37] Train loss : [608.3912702287946] Val Score : [0.04899551213734584])\n",
      "Epoch : [38] Train loss : [598.4751848493304] Val Score : [0.057788950251276595])\n",
      "Epoch : [39] Train loss : [588.3646065848214] Val Score : [0.06746717542724968])\n",
      "Epoch : [40] Train loss : [578.3718784877232] Val Score : [0.07905088420534415])\n",
      "Epoch : [41] Train loss : [568.3359026227679] Val Score : [0.09131533650167362])\n",
      "Epoch : [42] Train loss : [558.1765921456473] Val Score : [0.10456414504643781])\n",
      "Epoch : [43] Train loss : [548.1682477678571] Val Score : [0.12006877100275004])\n",
      "Epoch : [44] Train loss : [538.2360055106027] Val Score : [0.1366855961812415])\n",
      "Epoch : [45] Train loss : [528.341779436384] Val Score : [0.154413744025049])\n",
      "Epoch : [46] Train loss : [518.5699550083706] Val Score : [0.17262197793531944])\n",
      "Epoch : [47] Train loss : [508.8144313267299] Val Score : [0.19180369312730633])\n",
      "Epoch : [48] Train loss : [498.963143484933] Val Score : [0.209970789791594])\n",
      "Epoch : [49] Train loss : [489.1043047223772] Val Score : [0.22740672877195595])\n",
      "Epoch : [50] Train loss : [479.17645263671875] Val Score : [0.2442809210694559])\n",
      "Epoch : [51] Train loss : [469.14493233816967] Val Score : [0.26111500151521])\n",
      "Epoch : [52] Train loss : [459.0378853934152] Val Score : [0.27776493065184804])\n",
      "Epoch : [53] Train loss : [448.8570295061384] Val Score : [0.29470408744320314])\n",
      "Epoch : [54] Train loss : [438.6736057826451] Val Score : [0.3115773310416094])\n",
      "Epoch : [55] Train loss : [428.43067714146207] Val Score : [0.32744399825785675])\n",
      "Epoch : [56] Train loss : [418.28646414620533] Val Score : [0.3434538164803641])\n",
      "Epoch : [57] Train loss : [408.07190377371654] Val Score : [0.35835633096789254])\n",
      "Epoch : [58] Train loss : [398.12931169782365] Val Score : [0.3716181010901821])\n",
      "Epoch : [59] Train loss : [388.17647443498885] Val Score : [0.384574693111322])\n",
      "Epoch : [60] Train loss : [378.4310825892857] Val Score : [0.397519805082357])\n",
      "Epoch : [61] Train loss : [368.8882489885603] Val Score : [0.4092658976353072])\n",
      "Epoch : [62] Train loss : [359.5299987792969] Val Score : [0.41922291688136487])\n",
      "Epoch : [63] Train loss : [350.41402762276783] Val Score : [0.428554151177844])\n",
      "Epoch : [64] Train loss : [341.5205339704241] Val Score : [0.43700919458200777])\n",
      "Epoch : [65] Train loss : [333.0254647391183] Val Score : [0.44428267337725064])\n",
      "Epoch : [66] Train loss : [324.9088614327567] Val Score : [0.4503615598431344])\n",
      "Epoch : [67] Train loss : [317.15904453822543] Val Score : [0.45508518931057035])\n",
      "Epoch : [68] Train loss : [310.0525469098772] Val Score : [0.45875860082378966])\n",
      "Epoch : [69] Train loss : [303.37704031808033] Val Score : [0.46183218758125916])\n",
      "Epoch : [70] Train loss : [297.2006051199777] Val Score : [0.4642622302549401])\n",
      "Epoch : [71] Train loss : [291.2111162458147] Val Score : [0.46656106450713564])\n",
      "Epoch : [72] Train loss : [285.6590794154576] Val Score : [0.4685776234651231])\n",
      "Epoch : [73] Train loss : [280.44976806640625] Val Score : [0.4704706652042986])\n",
      "Epoch : [74] Train loss : [275.51686314174106] Val Score : [0.4722661026470294])\n",
      "Epoch : [75] Train loss : [270.7630135672433] Val Score : [0.4739438894236971])\n",
      "Epoch : [76] Train loss : [266.1467503138951] Val Score : [0.47591356616530406])\n",
      "Epoch : [77] Train loss : [261.65389142717635] Val Score : [0.4778827164563275])\n",
      "Epoch : [78] Train loss : [257.2184361049107] Val Score : [0.47970659835063933])\n",
      "Epoch : [79] Train loss : [252.96888078962053] Val Score : [0.4818627932493068])\n",
      "Epoch : [80] Train loss : [248.78941999162947] Val Score : [0.48379519217103617])\n",
      "Epoch : [81] Train loss : [244.78750610351562] Val Score : [0.4856666485538601])\n",
      "Epoch : [82] Train loss : [240.8736092703683] Val Score : [0.48722009819539414])\n",
      "Epoch : [83] Train loss : [237.08573695591517] Val Score : [0.488575215171623])\n",
      "Epoch : [84] Train loss : [233.49206760951452] Val Score : [0.48984819819373626])\n",
      "Epoch : [85] Train loss : [230.0465131487165] Val Score : [0.4908475462001816])\n",
      "Epoch : [86] Train loss : [226.8323931012835] Val Score : [0.49188114017357526])\n",
      "Epoch : [87] Train loss : [223.76325116838728] Val Score : [0.4930273715222723])\n",
      "Epoch : [88] Train loss : [220.87583923339844] Val Score : [0.49420306508247624])\n",
      "Epoch : [89] Train loss : [218.07010105678015] Val Score : [0.4953323591851261])\n",
      "Epoch : [90] Train loss : [215.43549019949776] Val Score : [0.4964287240054909])\n",
      "Epoch : [91] Train loss : [212.8564692905971] Val Score : [0.49774343787561404])\n",
      "Epoch : [92] Train loss : [210.34697614397322] Val Score : [0.4989850979779271])\n",
      "Epoch : [93] Train loss : [207.86895751953125] Val Score : [0.5004020107985073])\n",
      "Epoch : [94] Train loss : [205.53099278041296] Val Score : [0.5015404817673821])\n",
      "Epoch : [95] Train loss : [203.20103672572546] Val Score : [0.5022131846251776])\n",
      "Epoch : [96] Train loss : [200.88671221051897] Val Score : [0.5028814561952498])\n",
      "Epoch : [97] Train loss : [198.6558336530413] Val Score : [0.503708163582226])\n",
      "Epoch : [98] Train loss : [196.48885018484933] Val Score : [0.5042242224252992])\n",
      "Epoch : [99] Train loss : [194.36426435198103] Val Score : [0.5049202839008781])\n",
      "Epoch : [100] Train loss : [192.24352373395647] Val Score : [0.5054595406190284])\n",
      "Epoch : [101] Train loss : [190.24657113211495] Val Score : [0.5062105708069892])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [102] Train loss : [188.1869179861886] Val Score : [0.5065714272906093])\n",
      "Epoch : [103] Train loss : [186.20270211356026] Val Score : [0.5070295408296489])\n",
      "Epoch : [104] Train loss : [184.2627716064453] Val Score : [0.5075664862314123])\n",
      "Epoch : [105] Train loss : [182.34217834472656] Val Score : [0.5080666648195433])\n",
      "Epoch : [106] Train loss : [180.48748561314173] Val Score : [0.508625929326738])\n",
      "Epoch : [107] Train loss : [178.6505584716797] Val Score : [0.5091475667752355])\n",
      "Epoch : [108] Train loss : [176.80838666643416] Val Score : [0.509577923511901])\n",
      "Epoch : [109] Train loss : [175.01354108537947] Val Score : [0.5099121032616577])\n",
      "Epoch : [110] Train loss : [173.25343540736608] Val Score : [0.510303376299578])\n",
      "Epoch : [111] Train loss : [171.55914306640625] Val Score : [0.5107009582598172])\n",
      "Epoch : [112] Train loss : [169.83494785853796] Val Score : [0.5109966687836232])\n",
      "Epoch : [113] Train loss : [168.1860089983259] Val Score : [0.5114884236319622])\n",
      "Epoch : [114] Train loss : [166.48064967564173] Val Score : [0.5119059662115323])\n",
      "Epoch : [115] Train loss : [164.919921875] Val Score : [0.5122168149853599])\n",
      "Epoch : [116] Train loss : [163.32809230259485] Val Score : [0.5124741646056428])\n",
      "Epoch : [117] Train loss : [161.77535792759485] Val Score : [0.5128216460429015])\n",
      "Epoch : [118] Train loss : [160.25806318010603] Val Score : [0.5131742471769007])\n",
      "Epoch : [119] Train loss : [158.71670532226562] Val Score : [0.5135622067668206])\n",
      "Epoch : [120] Train loss : [157.23301260811942] Val Score : [0.514233537436889])\n",
      "Epoch : [121] Train loss : [155.79778398786272] Val Score : [0.5148290546031206])\n",
      "Epoch : [122] Train loss : [154.35341317313058] Val Score : [0.515440006945843])\n",
      "Epoch : [123] Train loss : [152.9570029122489] Val Score : [0.5159338133164293])\n",
      "Epoch : [124] Train loss : [151.62230573381697] Val Score : [0.5164722622477035])\n",
      "Epoch : [125] Train loss : [150.25293186732702] Val Score : [0.5171281923429423])\n",
      "Epoch : [126] Train loss : [148.95709010532923] Val Score : [0.517659362423344])\n",
      "Epoch : [127] Train loss : [147.72284807477678] Val Score : [0.5182395943755592])\n",
      "Epoch : [128] Train loss : [146.4921657017299] Val Score : [0.5187218979584981])\n",
      "Epoch : [129] Train loss : [145.32330540248327] Val Score : [0.5190998094691492])\n",
      "Epoch : [130] Train loss : [144.1753387451172] Val Score : [0.5195615379961316])\n",
      "Epoch : [131] Train loss : [143.03160749162947] Val Score : [0.5198351612363772])\n",
      "Epoch : [132] Train loss : [141.9329092843192] Val Score : [0.5202317017420814])\n",
      "Epoch : [133] Train loss : [140.81661333356584] Val Score : [0.5205133421516981])\n",
      "Epoch : [134] Train loss : [139.75986807686942] Val Score : [0.5210455680675096])\n",
      "Epoch : [135] Train loss : [138.75969805036272] Val Score : [0.5215478379345262])\n",
      "Epoch : [136] Train loss : [137.7633013044085] Val Score : [0.5218030702683638])\n",
      "Epoch : [137] Train loss : [136.7647988455636] Val Score : [0.5221043952670403])\n",
      "Epoch : [138] Train loss : [135.78020804268974] Val Score : [0.5227188978437106])\n",
      "Epoch : [139] Train loss : [134.8431876046317] Val Score : [0.5230322754335729])\n",
      "Epoch : [140] Train loss : [133.96158490862166] Val Score : [0.5233498762154124])\n",
      "Epoch : [141] Train loss : [133.04308864048548] Val Score : [0.5239044751074692])\n",
      "Epoch : [142] Train loss : [132.12845720563615] Val Score : [0.5243767796796488])\n",
      "Epoch : [143] Train loss : [131.248779296875] Val Score : [0.524761510660486])\n",
      "Epoch : [144] Train loss : [130.36861746651786] Val Score : [0.5251033319436452])\n",
      "Epoch : [145] Train loss : [129.53479548863] Val Score : [0.5253505267443636])\n",
      "Epoch : [146] Train loss : [128.69191414969308] Val Score : [0.525700955198524])\n",
      "Epoch : [147] Train loss : [127.82857949393136] Val Score : [0.5262622048180549])\n",
      "Epoch : [148] Train loss : [127.00360870361328] Val Score : [0.5267313678808235])\n",
      "Epoch : [149] Train loss : [126.16155678885323] Val Score : [0.5269960160358533])\n",
      "Epoch : [150] Train loss : [125.3465347290039] Val Score : [0.5273714616187877])\n",
      "Epoch : [151] Train loss : [124.57394191196987] Val Score : [0.527807788775894])\n",
      "Epoch : [152] Train loss : [123.8158438546317] Val Score : [0.528477313683947])\n",
      "Epoch : [153] Train loss : [123.02062007359096] Val Score : [0.528761792359579])\n",
      "Epoch : [154] Train loss : [122.25798688616071] Val Score : [0.5291075832574624])\n",
      "Epoch : [155] Train loss : [121.4951171875] Val Score : [0.5293995060019794])\n",
      "Epoch : [156] Train loss : [120.7379880632673] Val Score : [0.5297544391655133])\n",
      "Epoch : [157] Train loss : [120.06118774414062] Val Score : [0.5304799497789369])\n",
      "Epoch : [158] Train loss : [119.29796491350446] Val Score : [0.531038314631336])\n",
      "Epoch : [159] Train loss : [118.54985809326172] Val Score : [0.5317381185273695])\n",
      "Epoch : [160] Train loss : [117.85264042445591] Val Score : [0.532391626391109])\n",
      "Epoch : [161] Train loss : [117.15750994001117] Val Score : [0.5327919770339831])\n",
      "Epoch : [162] Train loss : [116.51943969726562] Val Score : [0.5336815796643095])\n",
      "Epoch : [163] Train loss : [115.82203020368304] Val Score : [0.5345310137650351])\n",
      "Epoch : [164] Train loss : [115.11336081368583] Val Score : [0.5348204675779968])\n",
      "Epoch : [165] Train loss : [114.4738300868443] Val Score : [0.5350396913539963])\n",
      "Epoch : [166] Train loss : [113.80699157714844] Val Score : [0.5357840109763295])\n",
      "Epoch : [167] Train loss : [113.07705579485211] Val Score : [0.5363178705739462])\n",
      "Epoch : [168] Train loss : [112.40682220458984] Val Score : [0.5368627250477384])\n",
      "Epoch : [169] Train loss : [111.75226266043526] Val Score : [0.5372588539994896])\n",
      "Epoch : [170] Train loss : [111.090087890625] Val Score : [0.5377420849055329])\n",
      "Epoch : [171] Train loss : [110.47028241838727] Val Score : [0.5380691350460268])\n",
      "Epoch : [172] Train loss : [109.88413783482143] Val Score : [0.5385672581308983])\n",
      "Epoch : [173] Train loss : [109.24718366350446] Val Score : [0.5389894609401439])\n",
      "Epoch : [174] Train loss : [108.66851370675224] Val Score : [0.5394183005621207])\n",
      "Epoch : [175] Train loss : [108.04092407226562] Val Score : [0.5405656326020356])\n",
      "Epoch : [176] Train loss : [107.44050053187779] Val Score : [0.5415748961876343])\n",
      "Epoch : [177] Train loss : [106.84087698800224] Val Score : [0.5423323635070021])\n",
      "Epoch : [178] Train loss : [106.30982862200055] Val Score : [0.5427188253653])\n",
      "Epoch : [179] Train loss : [105.7153570992606] Val Score : [0.5435077960045478])\n",
      "Epoch : [180] Train loss : [105.16276550292969] Val Score : [0.5441140534215738])\n",
      "Epoch : [181] Train loss : [104.61135755266461] Val Score : [0.5448377414221778])\n",
      "Epoch : [182] Train loss : [104.0394047328404] Val Score : [0.5457952670548532])\n",
      "Epoch : [183] Train loss : [103.47927638462612] Val Score : [0.5464512870526415])\n",
      "Epoch : [184] Train loss : [102.92083631243024] Val Score : [0.5468967903812862])\n",
      "Epoch : [185] Train loss : [102.38388497488839] Val Score : [0.5481570408842169])\n",
      "Epoch : [186] Train loss : [101.85964747837612] Val Score : [0.5491075105154902])\n",
      "Epoch : [187] Train loss : [101.34495762416294] Val Score : [0.550088207505037])\n",
      "Epoch : [188] Train loss : [100.84408569335938] Val Score : [0.5509723592989528])\n",
      "Epoch : [189] Train loss : [100.29524666922433] Val Score : [0.551488958700148])\n",
      "Epoch : [190] Train loss : [99.78597259521484] Val Score : [0.5521467507845381])\n",
      "Epoch : [191] Train loss : [99.26341574532645] Val Score : [0.5533658767406378])\n",
      "Epoch : [192] Train loss : [98.77153015136719] Val Score : [0.5543468000359598])\n",
      "Epoch : [193] Train loss : [98.29434640066964] Val Score : [0.5552115490246543])\n",
      "Epoch : [194] Train loss : [97.83050973074776] Val Score : [0.5562497050356316])\n",
      "Epoch : [195] Train loss : [97.33364759172711] Val Score : [0.5573209235700024])\n",
      "Epoch : [196] Train loss : [96.87719290597099] Val Score : [0.5584269155630615])\n",
      "Epoch : [197] Train loss : [96.36490413120815] Val Score : [0.5595695120153324])\n",
      "Epoch : [198] Train loss : [95.88832310267857] Val Score : [0.5609226813318595])\n",
      "Epoch : [199] Train loss : [95.43492998395648] Val Score : [0.5621505055417994])\n",
      "Epoch : [200] Train loss : [95.02301352364677] Val Score : [0.5626898219704392])\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "model = GANnomaly()\n",
    "model.encoder.train()\n",
    "model.decoder.train()\n",
    "model.discriminator.train()\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model.optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "for i in range(1,epochs+1) :\n",
    "        \n",
    "    train_loss = []\n",
    "    best_score = 0\n",
    "\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        \n",
    "        x = data.to(device)\n",
    "        \n",
    "        z_code = model.encoder(x)\n",
    "        x_hat = model.decoder(z_code)\n",
    "        z_code_hat = model.encoder(x_hat)\n",
    "        \n",
    "        dis_x, features_real = model.discriminator(x)\n",
    "        dis_x_hat, features_fake = model.discriminator(x_hat)\n",
    "        \n",
    "\n",
    "        l_tot, l_enc, l_con, l_adv = loss_gan(z_code, z_code_hat, \n",
    "                                       x, x_hat, \n",
    "                                       dis_x, dis_x_hat, \n",
    "                                       features_real, features_fake)\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "        l_tot.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        train_loss.append(l_tot.item())\n",
    "    \n",
    "    #validation\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "    \n",
    "    pred = []\n",
    "    true = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in iter(val_loader):\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _x = model.decoder(model.encoder(x))\n",
    "            diff = cos(x, _x).cpu().tolist()\n",
    "            batch_pred = np.where(np.array(diff) < 0.95, 1,0).tolist()\n",
    "            pred += batch_pred\n",
    "            true += y.tolist()\n",
    "        \n",
    "        score = f1_score(true, pred, average='macro')\n",
    "    \n",
    "    \n",
    "    print(f'Epoch : [{i}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "\n",
    "\n",
    "    scheduler.step(score)\n",
    "\n",
    "    if best_score < score:\n",
    "        best_score = score\n",
    "        torch.save(model.encoder.state_dict(), 'best_encoder.pth', _use_new_zipfile_serialization=False)\n",
    "        torch.save(model.decoder.state_dict(), 'best_decoder.pth', _use_new_zipfile_serialization=False)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdFdaj8gzBBV",
    "outputId": "827a7f9a-671a-43fb-ad2e-f781aa2a2bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Linear(in_features=64, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GANnomaly()\n",
    "model.encoder.load_state_dict(torch.load('best_encoder.pth'))\n",
    "model.decoder.load_state_dict(torch.load('best_decoder.pth'))\n",
    "model.encoder.eval()\n",
    "model.decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H3t6zM3DzCyn"
   },
   "outputs": [],
   "source": [
    "val_dataset = MyDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n_8IzryOzFkj"
   },
   "outputs": [],
   "source": [
    "model.encoder.eval()\n",
    "model.decoder.eval()\n",
    "val_sim = np.zeros(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in val_loader:\n",
    "        data, target = data.to(device), data.cpu()\n",
    "        output = model.decoder(model.encoder(data))\n",
    "        output = output.reshape(-1,30).cpu()\n",
    "        target = target.reshape(-1,30)\n",
    "        val_sim = np.append(val_sim, cos(output,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfCu-WQ2zGYu",
    "outputId": "7f4461a6-153a-49ec-9f9a-07377225c4bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9165787375726882\n",
      "[[28427     5]\n",
      " [    5    25]]\n"
     ]
    }
   ],
   "source": [
    "thr = 0.814\n",
    "\n",
    "pred_GAN = np.where(val_sim < thr,1,0)\n",
    "\n",
    "print(f1_score(pred_GAN,val_df['Class'],average='macro'))\n",
    "print(confusion_matrix(val_df['Class'],pred_GAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDIamu7czLLY",
    "outputId": "f4c5068c-cc51-456b-c217-9294147690d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28432\n",
      "           1       0.83      0.83      0.83        30\n",
      "\n",
      "    accuracy                           1.00     28462\n",
      "   macro avg       0.92      0.92      0.92     28462\n",
      "weighted avg       1.00      1.00      1.00     28462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_df['Class'],pred_GAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hZpagLd-gbt"
   },
   "source": [
    "## EllipticEnvelope\n",
    "\n",
    "*   사이킷런에서 제공하는 공분산 추정을 할 수 있는 객체입니다\n",
    "\n",
    "*   데이터의 모양을 정의하는데, 중심 데이터에 타원을 맞추고 그 바깥의 점을 무시하는 방식입니다\n",
    "\n",
    "*   인라이어 데이터가 정규 분포를 이룬다고 가정하면 인라이어 위치와 분산을 추정할 수 있고 이를 토대로 Mahalanobis 거리를 얻어 아웃라이어를 추정합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXKqCv1u-vYf",
    "outputId": "a8c49ebd-36a4-4b46-8b09-6a72264b4cc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010540369615627855"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = val_df['Class'].sum()/len(val_df)\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27uBVtGO-pA5",
    "outputId": "ca58e8a0-8637-4221-8e92-5a529d201945"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>EllipticEnvelope(contamination=0.0010540369615627855, random_state=2022,\n",
       "                 support_fraction=0.994)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EllipticEnvelope</label><div class=\"sk-toggleable__content\"><pre>EllipticEnvelope(contamination=0.0010540369615627855, random_state=2022,\n",
       "                 support_fraction=0.994)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "EllipticEnvelope(contamination=0.0010540369615627855, random_state=2022,\n",
       "                 support_fraction=0.994)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EE = EllipticEnvelope(support_fraction = 0.994, contamination = ratio, random_state = 2022)\n",
    "EE.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uoUzXTE-zF8",
    "outputId": "0735db41-6df5-4e03-a5d5-4efc288d4c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.9236496787663914]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28432\n",
      "           1       0.86      0.83      0.85        30\n",
      "\n",
      "    accuracy                           1.00     28462\n",
      "   macro avg       0.93      0.92      0.92     28462\n",
      "weighted avg       1.00      1.00      1.00     28462\n",
      "\n",
      "[[28428     4]\n",
      " [    5    25]]\n"
     ]
    }
   ],
   "source": [
    "def get_pred_label_EE(model, x, k):\n",
    "    prob = model.score_samples(x)\n",
    "    prob = torch.tensor(prob, dtype = torch.float)\n",
    "    topk_indices = torch.topk(prob, k = k, largest = False).indices\n",
    "\n",
    "    pred = torch.zeros(len(x), dtype = torch.long)\n",
    "    pred[topk_indices] = 1\n",
    "    return pred.tolist(), prob.tolist()\n",
    "\n",
    "val_x = val_df.drop(columns= 'Class') # Input Data\n",
    "val_y = val_df['Class'] # Label\n",
    "\n",
    "pred_EE, prob = get_pred_label_EE(EE, val_x, 29)\n",
    "val_score = f1_score(val_y, pred_EE, average='macro')\n",
    "print(f'Validation F1 Score : [{val_score}]')\n",
    "print(classification_report(val_y, pred_EE))\n",
    "print(confusion_matrix(val_y, pred_EE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEzMsKzVBO7D"
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CbhEVVhPBFX1"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mode (x) :\n",
    "    cnt = Counter(x)\n",
    "    mode = cnt.most_common(1)\n",
    "    return mode[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SqkNfBn1BHZw"
   },
   "outputs": [],
   "source": [
    "def get_ensemble_pred (x,) :\n",
    "    # pred GAN\n",
    "    test_dataset = MyDataset(x, False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "    test_sim = np.zeros(0)\n",
    "    print('start')\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data, target = data.to(device), data.cpu()\n",
    "            output = model.decoder(model.encoder(data))\n",
    "            output = output.reshape(-1,30).cpu()\n",
    "            target = target.reshape(-1,30)\n",
    "            test_sim = np.append(test_sim, cos(output,target))\n",
    "    print('GAN_done')\n",
    "    \n",
    "    pred_GAN = np.where(test_sim < 0.95,1,0)\n",
    "    # pred EE\n",
    "    k = 0.002231531967748047\n",
    "    pred_EE, prob = get_pred_label_EE(EE, x, int(len(x)*k))\n",
    "    print('EE_done')\n",
    "    preds = pd.DataFrame(zip(pred_GAN, pred_EE))\n",
    "    preds.columns = ['pred_GAN', 'pred_EE']\n",
    "    return preds, preds.apply(mode,axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIeCgoSJGrRp"
   },
   "source": [
    "## Method 2. Auto Encoder + GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "We1bwhpjGiFt"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('open/train.csv')\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "train_df[\"ABS Amount\"] = train_df.abs().sum(1)\n",
    "train_df[\"Amount\"] = train_df.sum(1)\n",
    "val_df = pd.read_csv('open/val.csv')\n",
    "val_df = val_df.drop(columns=['ID'])\n",
    "val_df[\"ABS Amount\"] = val_df.abs().sum(1)\n",
    "val_df[\"Amount\"] = val_df.sum(1)\n",
    "test_df = pd.read_csv('open/test.csv')\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_df[\"ABS Amount\"] = test_df.abs().sum(1)\n",
    "test_df[\"Amount\"] = test_df.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7Po6y52PGqKv"
   },
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "train = scaler.fit_transform(train_df)\n",
    "train = pd.DataFrame(train, columns = train_df.columns)\n",
    "val = scaler.transform(val_df.drop(columns = [\"Class\"]))\n",
    "val = pd.DataFrame(val, columns = [col for col in val_df.columns if col != \"Class\"])\n",
    "val[\"Class\"] = val_df[\"Class\"]\n",
    "test = scaler.transform(test_df)\n",
    "test = pd.DataFrame(test, columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "y0Leq-8fFauZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_size, z_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(input_size, 128)\n",
    "    self.linear2 = nn.Linear(128, 512)\n",
    "    self.linear3 = nn.Linear(512, z_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "        \n",
    "  def forward(self, real):\n",
    "    out = self.linear1(real)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear3(out)\n",
    "    z = self.relu(out)\n",
    "    return z\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, z_size, output_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(z_size, 512)\n",
    "    self.linear2 = nn.Linear(512, 128)\n",
    "    self.linear3 = nn.Linear(128, output_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "        \n",
    "  def forward(self, z):\n",
    "    out = self.linear1(z)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear3(out)\n",
    "    return out\n",
    "    \n",
    "class UsadModel(nn.Module):\n",
    "  def __init__(self, input_size, z_size):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(input_size, z_size)\n",
    "    self.decoder1 = Decoder(z_size, input_size)\n",
    "    self.decoder2 = Decoder(z_size, input_size)\n",
    "  \n",
    "  def training_step(self, batch, n):\n",
    "    z = self.encoder(batch)\n",
    "    w1 = self.decoder1(z)\n",
    "    w2 = self.decoder2(z)\n",
    "    w3 = self.decoder2(self.encoder(w1))\n",
    "    loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
    "    return loss1,loss2\n",
    "\n",
    "def training(epochs, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer1 = opt_func(list(model.encoder.parameters())+list(model.decoder1.parameters()))\n",
    "    optimizer2 = opt_func(list(model.encoder.parameters())+list(model.decoder2.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        for [batch] in train_loader:\n",
    "            batch=to_device(batch,device)\n",
    "            \n",
    "            #Train AE1\n",
    "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer1.zero_grad()\n",
    "            \n",
    "            \n",
    "            #Train AE2\n",
    "            loss1,loss2 = model.training_step(batch,epoch+1)\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "            optimizer2.zero_grad()\n",
    "\n",
    "        print(f\"Epoch : {epoch+1}\")\n",
    "        f1_score, threshold = calc_f1(model, val_loader)\n",
    "        torch.save(model.state_dict(), './usad_normal.pth', _use_new_zipfile_serialization=False)\n",
    "    return  threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ne2jxBcAFfyh"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RS9I23vWF0pM"
   },
   "outputs": [],
   "source": [
    "def calc_f1(model, val_loader):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    diffs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in iter(val_loader):\n",
    "            x = x.float().to(device)\n",
    "\n",
    "            _x = model.decoder1(model.encoder(x))\n",
    "            l1loss = nn.L1Loss(reduction = \"none\")\n",
    "            diff = l1loss(x, _x)\n",
    "            diff = torch.sum(diff, 1).cpu().numpy().tolist()\n",
    "            diffs.extend(diff)\n",
    "\n",
    "            # f1 score\n",
    "            true += y.tolist()\n",
    "    thresholds = []\n",
    "    f1_scores = []\n",
    "    for thr in np.linspace(0,100, 1000):\n",
    "      thresholds.append(thr)\n",
    "      pred = np.where(np.array(diffs)>thr, 1,0).tolist()\n",
    "      f1_scores.append(f1_score(true, pred, average='macro'))\n",
    "\n",
    "    max_f1 = max(f1_scores)\n",
    "    threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "    diffs = [[d] for d in diffs]\n",
    "    return max_f1, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EpGhIknGAKr",
    "outputId": "dde2c990-6003-4658-ccbe-1e9d428d4fb2"
   },
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "BATCH_SIZE =  256\n",
    "N_EPOCHS = 100\n",
    "input_size = 32\n",
    "z_size = 1024\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                                          data_utils.TensorDataset(torch.from_numpy(train.values).float()),\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=6\n",
    "                                          )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader( \n",
    "                                          data_utils.TensorDataset(torch.from_numpy(val.drop(columns=[\"Class\"]).values).float(), torch.from_numpy(val[\"Class\"].values).float()), \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=6\n",
    "                                          )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                                          data_utils.TensorDataset(torch.from_numpy(test.values).float()),\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=6\n",
    "                                          )\n",
    "\n",
    "model = UsadModel(input_size, z_size)\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJNN7p0ZGMcm",
    "outputId": "8c744e2f-7256-4296-f2e3-4f1abd62efb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Epoch : 2\n",
      "Epoch : 3\n",
      "Epoch : 4\n",
      "Epoch : 5\n",
      "Epoch : 6\n",
      "Epoch : 7\n",
      "Epoch : 8\n",
      "Epoch : 9\n",
      "Epoch : 10\n",
      "Epoch : 11\n",
      "Epoch : 12\n",
      "Epoch : 13\n",
      "Epoch : 14\n",
      "Epoch : 15\n",
      "Epoch : 16\n",
      "Epoch : 17\n",
      "Epoch : 18\n",
      "Epoch : 19\n",
      "Epoch : 20\n",
      "Epoch : 21\n",
      "Epoch : 22\n",
      "Epoch : 23\n",
      "Epoch : 24\n",
      "Epoch : 25\n",
      "Epoch : 26\n",
      "Epoch : 27\n",
      "Epoch : 28\n",
      "Epoch : 29\n",
      "Epoch : 30\n",
      "Epoch : 31\n",
      "Epoch : 32\n",
      "Epoch : 33\n",
      "Epoch : 34\n",
      "Epoch : 35\n",
      "Epoch : 36\n",
      "Epoch : 37\n",
      "Epoch : 38\n",
      "Epoch : 39\n",
      "Epoch : 40\n",
      "Epoch : 41\n",
      "Epoch : 42\n",
      "Epoch : 43\n",
      "Epoch : 44\n",
      "Epoch : 45\n",
      "Epoch : 46\n",
      "Epoch : 47\n",
      "Epoch : 48\n",
      "Epoch : 49\n",
      "Epoch : 50\n",
      "Epoch : 51\n",
      "Epoch : 52\n",
      "Epoch : 53\n",
      "Epoch : 54\n",
      "Epoch : 55\n",
      "Epoch : 56\n",
      "Epoch : 57\n",
      "Epoch : 58\n",
      "Epoch : 59\n",
      "Epoch : 60\n",
      "Epoch : 61\n",
      "Epoch : 62\n",
      "Epoch : 63\n",
      "Epoch : 64\n",
      "Epoch : 65\n",
      "Epoch : 66\n",
      "Epoch : 67\n",
      "Epoch : 68\n",
      "Epoch : 69\n",
      "Epoch : 70\n",
      "Epoch : 71\n",
      "Epoch : 72\n",
      "Epoch : 73\n",
      "Epoch : 74\n",
      "Epoch : 75\n",
      "Epoch : 76\n",
      "Epoch : 77\n",
      "Epoch : 78\n",
      "Epoch : 79\n",
      "Epoch : 80\n",
      "Epoch : 81\n",
      "Epoch : 82\n",
      "Epoch : 83\n",
      "Epoch : 84\n",
      "Epoch : 85\n",
      "Epoch : 86\n",
      "Epoch : 87\n",
      "Epoch : 88\n",
      "Epoch : 89\n",
      "Epoch : 90\n",
      "Epoch : 91\n",
      "Epoch : 92\n",
      "Epoch : 93\n",
      "Epoch : 94\n",
      "Epoch : 95\n",
      "Epoch : 96\n",
      "Epoch : 97\n",
      "Epoch : 98\n",
      "Epoch : 99\n",
      "Epoch : 100\n"
     ]
    }
   ],
   "source": [
    "threshold = training(N_EPOCHS,model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UDxpmEx2GPgk"
   },
   "outputs": [],
   "source": [
    "def prediction(model, thr, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    diffs = []\n",
    "    with torch.no_grad():\n",
    "        for [x] in iter(test_loader):\n",
    "\n",
    "            x = x.float().to(device)\n",
    "            _x = model.decoder1(model.encoder(x))\n",
    "            l1loss = nn.L1Loss(reduction = \"none\")\n",
    "            diff = l1loss(x, _x)\n",
    "            diff = torch.sum(diff, 1).cpu().numpy().tolist()\n",
    "            diffs.extend(diff)\n",
    "\n",
    "    pred = np.where(np.array(diffs)>thr, 1,0).tolist()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwhNTAf2GRG7",
    "outputId": "3eabae65-227c-4d4f-ba98-025fdd085201"
   },
   "outputs": [],
   "source": [
    "preds = prediction(model, 1.4, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
